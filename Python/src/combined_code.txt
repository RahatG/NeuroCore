from pydantic import BaseModel
from typing import List

class PredictResponse(BaseModel):
    prediction: List[float]

class TrainResponse(BaseModel):
    message: str
from pydantic import BaseModel

class PredictRequest(BaseModel):
    text: str

class TrainRequest(BaseModel):
    epochs: int
# src/models/__init__.py

from .user import User  # SQLAlchemy model
from .user_models import UserCreate, UserUpdate, UserInDB
from .nlp_models import (
    PredictRequest,
    PredictResponse,
    TrainRequest,
    TrainResponse,
    NERResponse,
    SentimentResponse,
    POSTaggerResponse,
)

__all__ = [
    "User",
    "UserCreate",
    "UserUpdate",
    "UserInDB",
    "PredictRequest",
    "PredictResponse",
    "TrainRequest",
    "TrainResponse",
    "NERResponse",
    "SentimentResponse",
    "POSTaggerResponse",
]
# src/models/user_models.py

from pydantic import BaseModel
from typing import List, Optional

# User-related Pydantic Models

class UserBase(BaseModel):
    username: str

class UserCreate(UserBase):
    password: str

class UserUpdate(UserBase):
    password: Optional[str] = None  # Allow password updates

class UserInDB(UserBase):
    id: int
    hashed_password: str

    model_config = {
        "from_attributes": True
    }

# NLP-related Pydantic Models (Consider moving these to nlp_models.py for clarity)

class PredictRequest(BaseModel):
    text: str

class PredictResponse(BaseModel):
    prediction: List[float]

class TrainRequest(BaseModel):
    epochs: int

class TrainResponse(BaseModel):
    message: str

class NERResponse(BaseModel):
    entities: List[dict]

class SentimentResponse(BaseModel):
    sentiment: float

class POSTaggerResponse(BaseModel):
    pos_tags: List[dict]
# src/models/user.py

from sqlalchemy import Column, Integer, String
from ..database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
# src/models/nlp_models.py

from pydantic import BaseModel
from typing import List, Optional

class PredictRequest(BaseModel):
    text: str  # The text you want to analyze

class PredictResponse(BaseModel):
    prediction: List[float]  # Example: Probability scores for each class

class TrainRequest(BaseModel):
    epochs: int  # Number of training epochs

class TrainResponse(BaseModel):
    message: str  # Confirmation message

class NERResponse(BaseModel):
    entities: List[dict]  # Named entities found

class SentimentResponse(BaseModel):
    sentiment: float  # Sentiment score

class POSTaggerResponse(BaseModel):
    pos_tags: List[dict]  # Part-of-speech tags
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
def health_check():
    return {"status": "Healthy"}
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from typing import List
from ..services.inference_service import InferenceService
from ..services.training_service import TrainingService
from ..services.nlp_service import NLPService

router = APIRouter()

class PredictRequest(BaseModel):
    text: str

class PredictResponse(BaseModel):
    prediction: List[float]

class TrainRequest(BaseModel):
    epochs: int

class TrainResponse(BaseModel):
    message: str

inference_service = InferenceService()
training_service = TrainingService()
nlp_service = NLPService()

@router.post("/predict", response_model=PredictResponse)
def predict(request: PredictRequest):
    prediction = inference_service.predict(request.text)
    return PredictResponse(prediction=prediction)

@router.post("/train", response_model=TrainResponse)
def train(request: TrainRequest):
    training_service.train(request.epochs)
    return TrainResponse(message=f"Training started for {request.epochs} epochs.")

cp cpp_backend/build/interface/InterfaceEngine.so /home/rahat/NeuroCore/Python/.venv/lib/python3.10/site-packages/
cpp_backend\build\interface

"\\wsl.localhost\Ubuntu\home\rahat\NeuroCore\cpp_backend\build\interface\InterfaceEngine.so"
# __init__.py
from .endpoints import router as endpoints_router
from .auth import router as auth_router
from .health_check import router as health_check_router

__all__ = ["endpoints_router", "auth_router", "health_check_router"]
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import jwt
import datetime
from ..config.settings import settings  # Ensure correct import path

router = APIRouter()

class Token(BaseModel):
    access_token: str
    token_type: str

class Login(BaseModel):
    username: str
    password: str

@router.post("/login", response_model=Token)
def login(login: Login):
    # Replace with actual user authentication logic
    if login.username == "admin" and login.password == "password":
        token = jwt.encode({
            "sub": login.username,
            "exp": datetime.datetime.utcnow() + datetime.timedelta(hours=2)
        }, settings.secret_key, algorithm=settings.algorithm)
        return {"access_token": token, "token_type": "bearer"}
    raise HTTPException(status_code=400, detail="Invalid credentials")
# Constants used across the application
API_V1_STR = "/api/v1"
# src/config/settings.py

from pydantic_settings import BaseSettings
from dotenv import load_dotenv
import os

# Load environment variables from .env file
dotenv_path = os.path.join(os.path.dirname(__file__), '../.env')
load_dotenv(dotenv_path)

class Settings(BaseSettings):
    data_path: str = "/home/rahat/NeuroCore/EAI_Model/arxiver/data/train.parquet"
    model_output_path: str = "/home/rahat/NeuroCore/EAI_Model/model"
    optimizer: str = "Adam"
    learning_rate: float = 0.001
    epochs: int = 10
    secret_key: str = "EC57D15924DF4B95344A47E95C2C5"
    algorithm: str = "HS256"
    host: str = "0.0.0.0"
    port: int = 8000
    log_level: str = "DEBUG"
    database_uri: str = "sqlite:///neurocore.db"

    model_config = {
        "from_attributes": True,
        "env_file": "../.env",
        "protected_namespaces": ("settings_",),
    }

settings = Settings()
print(f"Settings Loaded: {settings.dict()}")
# __init__.py
from .settings import Settings

settings = Settings()
# Database configuration
DATABASE_URI = "sqlite:///./neurocore.db"
from ..nlp.processor import Processor
from ..config.settings import settings

class NLPService:
    def __init__(self):
        self.processor = Processor(settings.data_path, settings.model_output_path)

    def perform_ner(self, tokens):
        return self.processor.engine.ner_recognize(tokens)

    def perform_sentiment_analysis(self, tokens):
        return self.processor.engine.sentiment_analyze(tokens)

    def perform_pos_tagging(self, tokens):
        return self.processor.engine.pos_tag(tokens)
from InterfaceEngine import DataLoader

class DataService:
    def __init__(self, data_path):
        self.loader = DataLoader(data_path)

    def get_data(self):
        return self.loader.load_data()
# __init__.py
from .data_service import DataService
from .inference_service import InferenceService
from .training_service import TrainingService
from .nlp_service import NLPService
from .user_service import UserService
from ..models.user_models import User, UserCreate, UserUpdate
from typing import List

class UserService:
    def __init__(self):
        self.users = {}

    def create_user(self, user: UserCreate):
        if user.username in self.users:
            raise Exception("User already exists.")
        self.users[user.username] = user.password

    def authenticate_user(self, username: str, password: str) -> bool:
        return self.users.get(username) == password

    def update_user(self, username: str, user: UserUpdate):
        if username not in self.users:
            raise Exception("User does not exist.")
        self.users[username] = user.password
from ..nlp.processor import Processor
from ..config.settings import settings

class InferenceService:
    def __init__(self):
        self.processor = Processor(settings.data_path, settings.model_output_path)

    def predict(self, text):
        return self.processor.process(text)
from ..interface.InterfaceEngine import InterfaceEngine
from ..config.settings import settings

class TrainingService:
    def __init__(self):
        self.engine = InterfaceEngine(settings.data_path, settings.model_output_path)

    def train(self, epochs):
        self.engine.train(epochs)
class Tokenizer:
    def tokenize(self, text):
        return text.split()
# __init__.py
from .processor import Processor
from .embedder import Embedder
from .tokenizer import Tokenizer
from ..interface.InterfaceEngine import InterfaceEngine

class Processor:
    def __init__(self, data_path, model_output_path):
        self.engine = InterfaceEngine(data_path, model_output_path)

    def process(self, text):
        return self.engine.predict(text)
import torch
from torch import nn
from ..config.settings import settings

class Embedder:
    def __init__(self, embedding_path):
        self.embedding = nn.EmbeddingBag.from_pretrained(torch.load(embedding_path), freeze=False)

    def get_embeddings(self, tokens):
        indices = [self.vocab[token] for token in tokens if token in self.vocab]
        tensor = torch.tensor(indices, dtype=torch.long)
        return self.embedding(tensor).detach().numpy()
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = None

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
import logging
from ..config.settings import settings

def get_logger(name: str):
    logger = logging.getLogger(name)
    logger.setLevel(settings.log_level)
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return logger
# src/database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from .config.settings import settings

engine = create_engine(settings.database_uri)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
# __init__.py
from .config import DATABASE_URI
from .database import engine, SessionLocal, Base
from .helpers import get_settings
from .logger import get_logger
from .security import get_current_user, oauth2_scheme
from .split import split_into_batches
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import jwt
from ..config.settings import settings

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, settings.secret_key, algorithms=[settings.algorithm])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid authentication credentials")
        return username
    except jwt.PyJWTError:
        raise HTTPException(status_code=401, detail="Invalid authentication credentials")
def split_into_batches(data, batch_size):
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]
from ..config.settings import settings

def get_settings():
    return settings
from fastapi import FastAPI, Depends, HTTPException
from pydantic import BaseModel
from typing import List
import InterfaceEngine  # C++ module import
from .auth import router as auth_router
from .endpoints import router as endpoints_router
from .health_check import router as health_check_router
from .config.settings import settings

app = FastAPI(title="NeuroCore AI API")

# Include Routers
app.include_router(auth_router, prefix="/api/v1")
app.include_router(endpoints_router, prefix="/api/v1")
app.include_router(health_check_router, prefix="/api/v1")

# Initialize InterfaceEngine (Singleton Pattern)
try:
    engine = InterfaceEngine.InterfaceEngine(settings.data_path, settings.model_output_path)
    print("InterfaceEngine initialized successfully.")
except Exception as e:
    print(f"Failed to initialize InterfaceEngine: {e}")
    raise e

# Define Pydantic Models
class PredictRequest(BaseModel):
    text: str

class PredictResponse(BaseModel):
    prediction: List[float]

class TrainRequest(BaseModel):
    epochs: int

class TrainResponse(BaseModel):
    message: str

# Prediction Endpoint
@app.post("/api/v1/predict", response_model=PredictResponse)
def predict(request: PredictRequest):
    try:
        prediction = engine.predict(request.text)
        return PredictResponse(prediction=prediction)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Training Endpoint
@app.post("/api/v1/train", response_model=TrainResponse)
def train(request: TrainRequest):
    try:
        engine.train(request.epochs)
        return TrainResponse(message=f"Training started for {request.epochs} epoch(s).")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
# inspect_parquet.py

import pyarrow.parquet as pq

def inspect_parquet(file_path):
    try:
        table = pq.read_table(file_path)
        print("Parquet File Schema:")
        print(table.schema)
        print("\nColumn Names:")
        for column in table.schema.names:
            print(f"- {column}")
    except Exception as e:
        print(f"Failed to read Parquet file: {e}")

if __name__ == "__main__":
    parquet_file = "/home/rahat/arxiver/data/train.parquet"  # Update this path if different
    inspect_parquet(parquet_file)
import InterfaceEngine

def main():
    try:
        # Initialize InterfaceEngine
        data_path = "/home/rahat/NeuroCore/EAI_Model/arxiver/data/train.parquet"
        model_output_path = "/home/rahat/NeuroCore/EAI_Model/model"
        engine = InterfaceEngine.InterfaceEngine(data_path, model_output_path)
        print("InterfaceEngine initialized successfully.")

        # Perform a prediction
        sample_text = "This is a sample input text for prediction."
        prediction = engine.predict(sample_text)
        print(f"Prediction: {prediction}")

        # Start training
        epochs = 1
        engine.train(epochs)
        print(f"Training completed for {epochs} epoch(s).")
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from .config.settings import settings

engine = create_engine(settings.database_uri)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
# src/init_db.py

from .database import engine, Base

def init_db():
    Base.metadata.create_all(bind=engine)
    print("Database tables created successfully.")

if __name__ == "__main__":
    init_db()
# src/init_db.py

from sqlalchemy.orm import Session
from .database import engine, Base
from .models.user import User

def init_db():
    Base.metadata.create_all(bind=engine)
    print("Database tables created successfully.")

if __name__ == "__main__":
    init_db()
