from pydantic import BaseModel
from typing import List

class PredictResponse(BaseModel):
    prediction: List[float]

class TrainResponse(BaseModel):
    message: str
from pydantic import BaseModel

class PredictRequest(BaseModel):
    text: str

class TrainRequest(BaseModel):
    epochs: int
# __init__.py
from .user_models import User, UserCreate, UserUpdate
from .nlp_models import PredictRequest, PredictResponse, TrainRequest, TrainResponse
from pydantic import BaseModel

class User(BaseModel):
    username: str

class UserCreate(BaseModel):
    username: str
    password: str

class UserUpdate(BaseModel):
    password: str
from pydantic import BaseModel
from typing import List

class NERResponse(BaseModel):
    entities: List[dict]

class SentimentResponse(BaseModel):
    sentiment: float

class POSTaggerResponse(BaseModel):
    pos_tags: List[dict]
    
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
def health_check():
    return {"status": "Healthy"}
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from typing import List
from ..services.inference_service import InferenceService
from ..services.training_service import TrainingService
from ..services.nlp_service import NLPService

router = APIRouter()

class PredictRequest(BaseModel):
    text: str

class PredictResponse(BaseModel):
    prediction: List[float]

class TrainRequest(BaseModel):
    epochs: int

class TrainResponse(BaseModel):
    message: str

inference_service = InferenceService()
training_service = TrainingService()
nlp_service = NLPService()

@router.post("/predict", response_model=PredictResponse)
def predict(request: PredictRequest):
    prediction = inference_service.predict(request.text)
    return PredictResponse(prediction=prediction)

@router.post("/train", response_model=TrainResponse)
def train(request: TrainRequest):
    training_service.train(request.epochs)
    return TrainResponse(message=f"Training started for {request.epochs} epochs.")

cp cpp_backend/build/interface/InterfaceEngine.so /home/rahat/NeuroCore/Python/.venv/lib/python3.10/site-packages/
cpp_backend\build\interface

"\\wsl.localhost\Ubuntu\home\rahat\NeuroCore\cpp_backend\build\interface\InterfaceEngine.so"
# __init__.py
from .endpoints import router as endpoints_router
from .auth import router as auth_router
from .health_check import router as health_check_router

__all__ = ["endpoints_router", "auth_router", "health_check_router"]
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import jwt
import datetime

router = APIRouter()

SECRET_KEY = "your_secret_key"

class Token(BaseModel):
    access_token: str
    token_type: str

class Login(BaseModel):
    username: str
    password: str

@router.post("/login", response_model=Token)
def login(login: Login):
    if login.username == "admin" and login.password == "password":
        token = jwt.encode({
            "sub": login.username,
            "exp": datetime.datetime.utcnow() + datetime.timedelta(hours=2)
        }, SECRET_KEY, algorithm="HS256")
        return {"access_token": token, "token_type": "bearer"}
    raise HTTPException(status_code=400, detail="Invalid credentials")
# Constants used across the application
API_V1_STR = "/api/v1"
from pydantic import BaseSettings

class Settings(BaseSettings):
    data_path: str
    model_output_path: str
    optimizer: str
    learning_rate: float
    epochs: int
    secret_key: str
    algorithm: str = "HS256"

    class Config:
        env_file = "../.env"

settings = Settings()
# __init__.py
from .settings import Settings

settings = Settings()
# Database configuration
DATABASE_URI = "sqlite:///./neurocore.db"
from ..nlp.processor import Processor
from ..config.settings import settings

class NLPService:
    def __init__(self):
        self.processor = Processor(settings.data_path, settings.model_output_path)

    def perform_ner(self, tokens):
        return self.processor.engine.ner_recognize(tokens)

    def perform_sentiment_analysis(self, tokens):
        return self.processor.engine.sentiment_analyze(tokens)

    def perform_pos_tagging(self, tokens):
        return self.processor.engine.pos_tag(tokens)
from InterfaceEngine import DataLoader

class DataService:
    def __init__(self, data_path):
        self.loader = DataLoader(data_path)

    def get_data(self):
        return self.loader.load_data()
# __init__.py
from .data_service import DataService
from .inference_service import InferenceService
from .training_service import TrainingService
from .nlp_service import NLPService
from .user_service import UserService
from ..models.user_models import User, UserCreate, UserUpdate
from typing import List

class UserService:
    def __init__(self):
        self.users = {}

    def create_user(self, user: UserCreate):
        if user.username in self.users:
            raise Exception("User already exists.")
        self.users[user.username] = user.password

    def authenticate_user(self, username: str, password: str) -> bool:
        return self.users.get(username) == password

    def update_user(self, username: str, user: UserUpdate):
        if username not in self.users:
            raise Exception("User does not exist.")
        self.users[username] = user.password
from ..nlp.processor import Processor
from ..config.settings import settings

class InferenceService:
    def __init__(self):
        self.processor = Processor(settings.data_path, settings.model_output_path)

    def predict(self, text):
        return self.processor.process(text)
from ..interface.InterfaceEngine import InterfaceEngine
from ..config.settings import settings

class TrainingService:
    def __init__(self):
        self.engine = InterfaceEngine(settings.data_path, settings.model_output_path)

    def train(self, epochs):
        self.engine.train(epochs)
class Tokenizer:
    def tokenize(self, text):
        return text.split()
# __init__.py
from .processor import Processor
from .embedder import Embedder
from .tokenizer import Tokenizer
from ..interface.InterfaceEngine import InterfaceEngine

class Processor:
    def __init__(self, data_path, model_output_path):
        self.engine = InterfaceEngine(data_path, model_output_path)

    def process(self, text):
        return self.engine.predict(text)
import torch
from torch import nn
from ..config.settings import settings

class Embedder:
    def __init__(self, embedding_path):
        self.embedding = nn.EmbeddingBag.from_pretrained(torch.load(embedding_path), freeze=False)

    def get_embeddings(self, tokens):
        indices = [self.vocab[token] for token in tokens if token in self.vocab]
        tensor = torch.tensor(indices, dtype=torch.long)
        return self.embedding(tensor).detach().numpy()
import logging
from ..config.settings import settings

def get_logger(name: str):
    logger = logging.getLogger(name)
    logger.setLevel(settings.log_level)
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return logger
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from ..config.settings import settings

SQLALCHEMY_DATABASE_URL = settings.database_uri

engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
# __init__.py
from .config import DATABASE_URI
from .database import engine, SessionLocal, Base
from .helpers import get_settings
from .logger import get_logger
from .security import get_current_user, oauth2_scheme
from .split import split_into_batches
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import jwt
from ..config.settings import settings

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, settings.secret_key, algorithms=[settings.algorithm])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid authentication credentials")
        return username
    except jwt.PyJWTError:
        raise HTTPException(status_code=401, detail="Invalid authentication credentials")
def split_into_batches(data, batch_size):
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]
from ..config.settings import settings

def get_settings():
    return settings
from fastapi import FastAPI
from .api import endpoints, auth, health_check
from .config.constants import API_V1_STR

app = FastAPI(title="NeuroCore AI API")

app.include_router(auth.router, prefix="/api/v1")
app.include_router(endpoints.router, prefix="/api/v1")
app.include_router(health_check.router, prefix="/api/v1")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


# inspect_parquet.py

import pyarrow.parquet as pq

def inspect_parquet(file_path):
    try:
        table = pq.read_table(file_path)
        print("Parquet File Schema:")
        print(table.schema)
        print("\nColumn Names:")
        for column in table.schema.names:
            print(f"- {column}")
    except Exception as e:
        print(f"Failed to read Parquet file: {e}")

if __name__ == "__main__":
    parquet_file = "/home/rahat/arxiver/data/train.parquet"  # Update this path if different
    inspect_parquet(parquet_file)
# test_interface_engine.py

from InterfaceEngine import DataLoader, InterfaceEngine

def main():
    # Initialize DataLoader with a sample data path
    data_path = "/home/rahat/arxiver/data/train.parquet"  # Ensure this path is correct
    try:
        data_loader = DataLoader(data_path)
        data = data_loader.load_data()
        print(f"Loaded {len(data)} data entries.")
    except Exception as e:
        print(f"DataLoader failed: {e}")
        return

    # Initialize InterfaceEngine with data and model output paths
    model_output_path = "/home/rahat/NeuroCore/Model/"  # Ensure this path is correct
    try:
        engine = InterfaceEngine(data_path, model_output_path)
        print("InterfaceEngine initialized successfully.")
    except Exception as e:
        print(f"InterfaceEngine initialization failed: {e}")
        return

    # Test the predict method
    sample_text = "This is a sample input text for prediction."
    try:
        predictions = engine.predict(sample_text)
        print(f"Predictions for '{sample_text}': {predictions}")
    except Exception as e:
        print(f"Prediction failed: {e}")

    # Test the train method
    epochs = 1  # Start with 1 epoch for testing
    try:
        engine.train(epochs)
        print(f"Training completed for {epochs} epoch(s).")
    except Exception as e:
        print(f"Training failed: {e}")

if __name__ == "__main__":
    main()
# __init__.py
# This file makes the src directory a Python package
