#include <pybind11/pybind11.h>
#include <pybind11/stl.h>  // Enable automatic conversion for STL containers
#include "InterfaceEngine.h"
#include "data/DataLoader.h"

namespace py = pybind11;

PYBIND11_MODULE(InterfaceEngine, m) {
    m.doc() = "InterfaceEngine Python bindings";

    // Register custom exceptions
    py::register_exception<std::invalid_argument>(m, "InvalidArgumentError");
    py::register_exception<std::runtime_error>(m, "RuntimeError");

    // Expose DataLoader
    py::class_<DataLoader>(m, "DataLoader")
        .def(py::init<const std::string&>(), py::arg("data_path"))
        .def("load_data", &DataLoader::load_data);

    // Expose InterfaceEngine
    py::class_<InterfaceEngine>(m, "InterfaceEngine")
        .def(py::init<const std::string&, const std::string&>(),
             py::arg("data_path"),
             py::arg("model_output_path"))
        .def("predict", &InterfaceEngine::predict)
        .def("train", &InterfaceEngine::train);
}
#ifndef INTERFACEENGINE_H
#define INTERFACEENGINE_H

#include <string>
#include <vector>
#include "../model/NeuralNetwork.h"
#include "../nlp/NLPProcessor.h"

class InterfaceEngine {
public:
    InterfaceEngine(const std::string& data_path, const std::string& model_output_path);
    std::vector<float> predict(const std::string& input_text);
    void train(int epochs);

private:
    NeuralNetwork network;
    NLPProcessor nlp_processor;

    // Declare the training data member variable
    std::vector<std::string> training_data;


    // Declare the member functions used in InterfaceEngine.cpp
    std::vector<float> get_target_for_data(const std::string& data);
    float calculate_loss(const std::vector<float>& predictions, const std::vector<float>& targets);
    std::vector<float> calculate_gradient(const std::vector<float>& predictions, const std::vector<float>& targets);
};

#endif // INTERFACEENGINE_H
#include "InterfaceEngine.h"
#include "data/DataLoader.h"
#include "model/NeuralNetwork.h"
#include "training/LossFunctions/CrossEntropyLoss.h"
#include "training/Metrics/Accuracy.h"
#include "utils/Logger.h"
#include "nlp/NLPProcessor.h"
#include <complex>

// Constructor
InterfaceEngine::InterfaceEngine(const std::string& data_path, const std::string& model_output_path)
        : network(), nlp_processor() {
    // Initialize with data and model paths
    DataLoader data_loader(data_path);
    training_data = data_loader.load_data();
}

// Predict method
std::vector<float> InterfaceEngine::predict(const std::string& input_text) {
    std::vector<std::string> tokens = nlp_processor.process_text(input_text);
    std::vector<float> input_vector(tokens.size(), 0.0f);
    for (size_t i = 0; i < tokens.size(); ++i) {
        input_vector[i] = static_cast<float>(tokens[i].length()); // Example conversion
    }
    return network.forward(input_vector);
}

// Train method
void InterfaceEngine::train(int epochs) {
    Logger logger(INFO);
    CrossEntropyLoss loss_fn;
    Accuracy metric_fn;

    for (int epoch = 0; epoch < epochs; ++epoch) {
        for (const auto& data : training_data) {
            std::vector<std::string> tokens = nlp_processor.process_text(data);
            std::vector<float> input_vector(tokens.size(), 0.0f);
            for (size_t i = 0; i < tokens.size(); ++i) {
                input_vector[i] = static_cast<float>(tokens[i].length()); // Example conversion
            }
            std::vector<float> output = network.forward(input_vector);

            // Calculate loss and perform backpropagation
            std::vector<float> target = get_target_for_data(data); // Assume this function provides the target output
            float loss = calculate_loss(output, target); // Calculate the loss
            std::vector<float> grad_output = calculate_gradient(output, target); // Calculate the gradient of the loss
            network.backward(grad_output);

            // Update network parameters
            network.update_parameters();
        }
        logger.log("Epoch " + std::to_string(epoch + 1) + " completed.");
    }
}

// Helper function to get target vectors
std::vector<float> InterfaceEngine::get_target_for_data(const std::string& data) {
    // Implement logic to convert data to target vectors
    // For example, one-hot encoding for classification tasks
    std::vector<float> target(10, 0.0f); // Assuming 10 classes
    // Set the appropriate index to 1 based on the data label
    // Example: target[3] = 1.0f;
    return target;
}

// Calculate loss using Cross-Entropy
float InterfaceEngine::calculate_loss(const std::vector<float>& predictions, const std::vector<float>& targets) {
    // Implement loss calculation, e.g., Cross-Entropy Loss
    float loss = 0.0f;
    for(size_t i = 0; i < predictions.size(); ++i) {
        loss -= targets[i] * std::log(predictions[i] + 1e-15f);
    }
    return loss;
}

// Calculate gradient of the loss
std::vector<float> InterfaceEngine::calculate_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) {
    // Implement gradient calculation for the loss function
    std::vector<float> grad(predictions.size(), 0.0f);
    for(size_t i = 0; i < predictions.size(); ++i) {
        grad[i] = -targets[i] / (predictions[i] + 1e-15f);
    }
    return grad;
}
#ifndef CROSSENTROPYLOSS_H
#define CROSSENTROPYLOSS_H

#include "LossFunction.h"

class CrossEntropyLoss : public LossFunction {
public:
    float compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) override;
    std::vector<float> compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // CROSSENTROPYLOSS_H
//
// Created by Rahat on 09/11/2024.
//

#include "LossFunction.h"
#ifndef MEANSQUAREDLOSS_H
#define MEANSQUAREDLOSS_H

#include "LossFunction.h"

class MeanSquaredLoss : public LossFunction {
public:
    float compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) override;
    std::vector<float> compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // MEANSQUAREDLOSS_H
#include "CrossEntropyLoss.h"
#include <cmath>

float CrossEntropyLoss::compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) {
    float loss = 0.0f;
    for(size_t i = 0; i < predictions.size(); ++i) {
        loss -= targets[i] * std::log(predictions[i] + 1e-15f);
    }
    return loss;
}

std::vector<float> CrossEntropyLoss::compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) {
    std::vector<float> grad(predictions.size(), 0.0f);
    for(size_t i = 0; i < predictions.size(); ++i) {
        grad[i] = -targets[i] / (predictions[i] + 1e-15f);
    }
    return grad;
}
#include <cstddef> // Add this line
#include "MeanSquaredLoss.h"
#include <cmath>

float MeanSquaredLoss::compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) {
    float loss = 0.0f;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        float diff = predictions[i] - targets[i];
        loss += diff * diff;
    }
    return loss / predictions.size();
}

std::vector<float> MeanSquaredLoss::compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) {
    std::vector<float> grad(predictions.size(), 0.0f);
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        grad[i] = 2.0f * (predictions[i] - targets[i]) / predictions.size();
    }
    return grad;
}
#ifndef LOSSFUNCTION_H
#define LOSSFUNCTION_H

#include <vector>

class LossFunction {
public:
    virtual float compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) = 0;
    virtual std::vector<float> compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) = 0;
};

#endif // LOSSFUNCTION_H
#include <cstddef> // Add this line
#include "Recall.h"

float Recall::compute(const std::vector<float>& predictions, const std::vector<float>& targets) {
    int true_positive = 0;
    int actual_positive = 0;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        if (targets[i] > 0.5f) {
            actual_positive++;
            if (predictions[i] > 0.5f) {
                true_positive++;
            }
        }
    }
    return actual_positive > 0 ? static_cast<float>(true_positive) / actual_positive : 0.0f;
}
#include <cstddef> // Add this line
#include "Accuracy.h"

float Accuracy::compute(const std::vector<float>& predictions, const std::vector<float>& targets) {
    int correct = 0;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        if ((predictions[i] > 0.5f) == (targets[i] > 0.5f)) {
            correct++;
        }
    }
    return static_cast<float>(correct) / predictions.size();
}
#ifndef METRIC_H
#define METRIC_H

#include <vector>

class Metric {
public:
    virtual float compute(const std::vector<float>& predictions, const std::vector<float>& targets) = 0;
};

#endif // METRIC_H
//
// Created by Rahat on 09/11/2024.
//

#include "Metric.h"
#include <cstddef> // Add this line
#include "Precision.h"

float Precision::compute(const std::vector<float>& predictions, const std::vector<float>& targets) {
    int true_positive = 0;
    int predicted_positive = 0;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        if (predictions[i] > 0.5f) {
            predicted_positive++;
            if (targets[i] > 0.5f) {
                true_positive++;
            }
        }
    }
    return predicted_positive > 0 ? static_cast<float>(true_positive) / predicted_positive : 0.0f;
}
#ifndef RECALL_H
#define RECALL_H

#include "Metric.h"

class Recall : public Metric {
public:
    float compute(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // RECALL_H
#ifndef ACCURACY_H
#define ACCURACY_H

#include "Metric.h"

class Accuracy : public Metric {
public:
    float compute(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // ACCURACY_H
#ifndef PRECISION_H
#define PRECISION_H

#include "Metric.h"

class Precision : public Metric {
public:
    float compute(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // PRECISION_H
#ifndef TRAINER_H
#define TRAINER_H

#include "../model/NeuralNetwork.h"
#include "LossFunctions/LossFunction.h"
#include "LossFunctions/CrossEntropyLoss.h"
#include "LossFunctions/MeanSquaredLoss.h"
#include "Metrics/Metric.h"
#include "Metrics/Accuracy.h"
#include "../data/DataLoader.h"
#include "../nlp/NLPProcessor.h"
#include <vector>
#include <string>

class Trainer {
public:
    Trainer(NeuralNetwork& network, LossFunction& loss_fn, Metric& metric, DataLoader& data_loader, NLPProcessor& nlp_processor);
    void train(int epochs);
private:
    NeuralNetwork& net;
    LossFunction& loss;
    Metric& metric_fn;
    DataLoader& loader;
    NLPProcessor& nlp;
};

#endif // TRAINER_H
#include "Trainer.h"
#include "../utils/Logger.h"
#include "../nlp/NLPProcessor.h"

Trainer::Trainer(NeuralNetwork& network, LossFunction& loss_fn, Metric& metric, DataLoader& data_loader, NLPProcessor& nlp_processor)
    : net(network), loss(loss_fn), metric_fn(metric), loader(data_loader), nlp(nlp_processor) {}

void Trainer::train(int epochs) {
    Logger logger(INFO);
    for(int epoch = 1; epoch <= epochs; ++epoch) {
        float epoch_loss = 0.0f;
        float epoch_metric = 0.0f;
        std::vector<std::string> data = loader.load_data();
        for(const auto &text : data) {
            std::vector<std::string> tokens = nlp.process_text(text);
            // Placeholder: Convert tokens to input vector
            std::vector<float> input(100, 1.0f); // Example input
            std::vector<float> targets(10, 0.0f); // Example target
            input = net.forward(input);
            float current_loss = loss.compute_loss(input, targets);
            epoch_loss += current_loss;
            std::vector<float> grad = loss.compute_gradient(input, targets);
            net.backward(grad);
            net.update_parameters();
            epoch_metric += metric_fn.compute(input, targets);
        }
        logger.log("Epoch " + std::to_string(epoch) + " Loss: " + std::to_string(epoch_loss / data.size()) + " Metric: " + std::to_string(epoch_metric / data.size()));
    }
}
#include "EmbeddingLoader.h"
#include <fstream>
#include <sstream>
#include <stdexcept>

EmbeddingLoader::EmbeddingLoader(const std::string& embedding_path) {
    load_embeddings(embedding_path);
}

void EmbeddingLoader::load_embeddings(const std::string& path) {
    std::ifstream infile(path);
    if (!infile.is_open()) {
        throw std::runtime_error("Failed to open embedding file.");
    }
    std::string line;
    while (std::getline(infile, line)) {
        std::istringstream iss(line);
        std::string word;
        iss >> word;
        std::vector<float> vec;
        float val;
        while (iss >> val) {
            vec.push_back(val);
        }
        embeddings[word] = vec;
    }
    infile.close();
}

std::vector<float> EmbeddingLoader::get_embedding(const std::string& word) {
    if (embeddings.find(word) != embeddings.end()) {
        return embeddings[word];
    }
    return std::vector<float>();
}
#ifndef WORDEMBEDDINGS_H
#define WORDEMBEDDINGS_H

#include "EmbeddingLoader.h"
#include <vector>
#include <string>

class WordEmbeddings {
public:
    WordEmbeddings(const std::string& embedding_path);
    std::vector<float> get_word_embedding(const std::string& word);
private:
    EmbeddingLoader loader;
};

#endif // WORDEMBEDDINGS_H
#include "WordEmbeddings.h"

WordEmbeddings::WordEmbeddings(const std::string& embedding_path)
    : loader(embedding_path) {}

std::vector<float> WordEmbeddings::get_word_embedding(const std::string& word) {
    return loader.get_embedding(word);
}
#ifndef EMBEDDINGLOADER_H
#define EMBEDDINGLOADER_H

#include <string>
#include <unordered_map>
#include <vector>

class EmbeddingLoader {
public:
    EmbeddingLoader(const std::string& embedding_path);
    std::vector<float> get_embedding(const std::string& word);
private:
    std::unordered_map<std::string, std::vector<float>> embeddings;
    void load_embeddings(const std::string& path);
};

#endif // EMBEDDINGLOADER_H
#ifndef TEXTCLEANER_H
#define TEXTCLEANER_H

#include <string>

class TextCleaner {
public:
    TextCleaner();
    std::string clean_text(const std::string& text);
private:
    void to_lowercase(std::string& text);
    void remove_punctuation(std::string& text);
};

#endif // TEXTCLEANER_H
// Lemmatiser.cpp
#include "Lemmatiser.h"
#include <algorithm>

Lemmatiser::Lemmatiser() {
    // Initialize lemmatizer with a simple dictionary
    lemma_dict = {
            {"running", "run"},
            {"jumps", "jump"},
            {"easily", "easy"},
            // Add more word-lemma pairs as needed
    };
}

std::string Lemmatiser::lemmatize(const std::string& word) const {
    std::string lower_word = word;
    std::transform(lower_word.begin(), lower_word.end(), lower_word.begin(), ::tolower);

    auto it = lemma_dict.find(lower_word);
    if (it != lemma_dict.end()) {
        return it->second;
    }
    return lower_word;
}

void Lemmatiser::addLemma(const std::string& word, const std::string& lemma) {
    std::string lower_word = word;
    std::transform(lower_word.begin(), lower_word.end(), lower_word.begin(),
                   [](unsigned char c) { return std::tolower(c); });
    lemma_dict[lower_word] = lemma;
}
#include "TextCleaner.h"
#include <algorithm>
#include <cctype>

TextCleaner::TextCleaner() {}

void TextCleaner::to_lowercase(std::string& text) {
    std::transform(text.begin(), text.end(), text.begin(),
                   [](unsigned char c){ return std::tolower(c); });
}

void TextCleaner::remove_punctuation(std::string& text) {
    text.erase(std::remove_if(text.begin(), text.end(),
        [](unsigned char c) { return std::ispunct(c); }), text.end());
}

std::string TextCleaner::clean_text(const std::string& text) {
    std::string cleaned = text;
    to_lowercase(cleaned);
    remove_punctuation(cleaned);
    return cleaned;
}
// Lemmatiser.h
#ifndef LEMMATISER_H
#define LEMMATISER_H

#include <string>
#include <unordered_map>

class Lemmatiser {
public:
    Lemmatiser();
    std::string lemmatize(const std::string& word) const;
    void addLemma(const std::string& word, const std::string& lemma);

private:
    std::unordered_map<std::string, std::string> lemma_dict;
};

#endif // LEMMATISER_H
#ifndef STOPWORDSREMOVER_H
#define STOPWORDSREMOVER_H

#include <string>
#include <vector>
#include <unordered_set>

class StopWordsRemover {
public:
    StopWordsRemover();
    std::vector<std::string> remove_stopwords(const std::vector<std::string>& words);
private:
    std::unordered_set<std::string> stopwords;
    void load_stopwords();
};

#endif // STOPWORDSREMOVER_H
#include "StopWordsRemover.h"

StopWordsRemover::StopWordsRemover() {
    load_stopwords();
}

void StopWordsRemover::load_stopwords() {
    stopwords = {"a", "an", "the", "and", "or", "but", "if", "while", "with", "to", "of"};
}

std::vector<std::string> StopWordsRemover::remove_stopwords(const std::vector<std::string>& words) {
    std::vector<std::string> filtered;
    for (const auto& word : words) {
        if (stopwords.find(word) == stopwords.end()) {
            filtered.push_back(word);
        }
    }
    return filtered;
}
#include "Tokenizer.h"
#include <sstream>

Tokenizer::Tokenizer() {}

std::vector<std::string> Tokenizer::tokenize(const std::string& text) {
    std::vector<std::string> tokens;
    std::istringstream iss(text);
    std::string token;
    while (iss >> token) {
        tokens.push_back(token);
    }
    return tokens;
}
#ifndef DATALOADER_H
#define DATALOADER_H

#include <string>
#include <vector>

class DataLoader {
public:
    DataLoader(const std::string& data_path);
    std::vector<std::string> load_data(); // Changed from vector<unordered_map>
private:
    std::string data_path;
};

#endif // DATALOADER_H
#include "DataLoader.h"
#include <arrow/api.h>
#include <arrow/io/api.h>
#include <parquet/arrow/reader.h>
#include <stdexcept>
#include <iostream>

DataLoader::DataLoader(const std::string& data_path) : data_path(data_path) {}

std::vector<std::string> DataLoader::load_data() {
    // Initialize Arrow's memory pool
    arrow::MemoryPool* pool = arrow::default_memory_pool();

    // Open the Parquet file
    std::shared_ptr<arrow::io::ReadableFile> infile;
    auto open_result = arrow::io::ReadableFile::Open(data_path, pool);
    if (!open_result.ok()) {
        std::cerr << "Failed to open data file at: " << data_path 
                  << "\nError: " << open_result.status().ToString() << std::endl;
        throw std::runtime_error("Failed to open data file.");
    }
    infile = *open_result;

    // Create Parquet reader
    std::unique_ptr<parquet::arrow::FileReader> reader;
    arrow::Status reader_result = parquet::arrow::OpenFile(infile, pool, &reader);
    if (!reader_result.ok()) {
        std::cerr << "Failed to create Parquet reader.\nError: " << reader_result.ToString() << std::endl;
        throw std::runtime_error("Failed to create Parquet reader.");
    }

    // Read the entire file into a table
    std::shared_ptr<arrow::Table> table;
    arrow::Status table_result = reader->ReadTable(&table);
    if (!table_result.ok()) {
        std::cerr << "Failed to read Parquet table.\nError: " << table_result.ToString() << std::endl;
        throw std::runtime_error("Failed to read Parquet table.");
    }

    // Extract data from the table
    std::vector<std::string> data;

    auto schema = table->schema();
    int num_columns = schema->num_fields();
    std::vector<std::string> column_names;
    for (int i = 0; i < num_columns; ++i) {
        column_names.emplace_back(schema->field(i)->name());
    }

    // Specify the target column
    std::string target_column = "text"; // Replace with your actual column name
    int target_col_index = -1;
    for (int i = 0; i < num_columns; ++i) {
        if (column_names[i] == target_column) {
            target_col_index = i;
            break;
        }
    }

    if (target_col_index == -1) {
        std::cerr << "DataLoader Error: Target column '" << target_column << "' not found in Parquet file." << std::endl;
        throw std::runtime_error("Target column not found in Parquet file.");
    }

    auto column = table->column(target_col_index)->chunk(0);
    auto string_array = std::static_pointer_cast<arrow::StringArray>(column);

    int num_rows = table->num_rows();
    for (int row = 0; row < num_rows; ++row) {
        if (!string_array->IsNull(row)) {
            data.emplace_back(string_array->GetString(row));
        } else {
            data.emplace_back("");
        }
    }

    std::cout << "DataLoader: Successfully loaded " << data.size() << " entries." << std::endl;
    return data;
}
#ifndef TOKENIZER_H
#define TOKENIZER_H

#include <string>
#include <vector>

class Tokenizer {
public:
    Tokenizer();
    std::vector<std::string> tokenize(const std::string& text);
};

#endif // TOKENIZER_H
#ifndef SENTIMENTANALYSIS_H
#define SENTIMENTANALYSIS_H

#include <vector>
#include <string>

class SentimentAnalysis {
public:
    SentimentAnalysis();
    float analyze(const std::vector<std::string>& tokens);
};

#endif // SENTIMENTANALYSIS_H
#ifndef NAMEDENTITYRECOGNITION_H
#define NAMEDENTITYRECOGNITION_H

#include <vector>
#include <string>

class NamedEntityRecognition {
public:
    NamedEntityRecognition();
    std::vector<std::pair<std::string, std::string>> recognize(const std::vector<std::string>& tokens);
};

#endif // NAMEDENTITYRECOGNITION_H
#include "SentimentAnalysis.h"

SentimentAnalysis::SentimentAnalysis() {}

float SentimentAnalysis::analyze(const std::vector<std::string>& tokens) {
    (void)tokens; // Mark as intentionally unused

    // Placeholder: Implement sentiment analysis logic
    return 0.0f;
}
#include "PartOfSpeechTagger.h"

std::vector<std::pair<std::string, std::string>> PartOfSpeechTagger::tag(const std::vector<std::string>& tokens) {
    (void)tokens; // Mark as intentionally unused

    // Placeholder: Implement POS tagging logic
    return std::vector<std::pair<std::string, std::string>>();
}
#include "NamedEntityRecognition.h"

std::vector<std::pair<std::string, std::string>> NamedEntityRecognition::recognize(const std::vector<std::string>& tokens) {
    (void)tokens; // Mark as intentionally unused

    // Placeholder: Implement NER logic
    return std::vector<std::pair<std::string, std::string>>();
}
#include "NLPProcessor.h"
#include "../data/Preprocessing/TextCleaner.h"
#include "../data/Preprocessing/StopWordsRemover.h"
#include "../data/Preprocessing/Lemmatiser.h"
#include "../data/Tokenizer.h"

NLPProcessor::NLPProcessor() {}

std::vector<std::string> NLPProcessor::process_text(const std::string& text) {
    TextCleaner cleaner;
    StopWordsRemover remover;
    Lemmatiser lemmatiser;
    Tokenizer tokenizer;

    std::string cleaned = cleaner.clean_text(text);
    std::vector<std::string> tokens = tokenizer.tokenize(cleaned);
    tokens = remover.remove_stopwords(tokens);
    std::vector<std::string> lemmatized;
    for(auto &token : tokens) {
        lemmatized.push_back(lemmatiser.lemmatize(token));
    }
    return lemmatized;
}
#ifndef NLPPROCESSOR_H
#define NLPPROCESSOR_H

#include <string>
#include <vector>

class NLPProcessor {
public:
    NLPProcessor();
    std::vector<std::string> process_text(const std::string& text);
private:
    // Other members and methods
};

#endif // NLPPROCESSOR_H
#ifndef PARTOFSPEECHTAGGER_H
#define PARTOFSPEECHTAGGER_H

#include <vector>
#include <string>

class PartOfSpeechTagger {
public:
    PartOfSpeechTagger();
    std::vector<std::pair<std::string, std::string>> tag(const std::vector<std::string>& tokens);
};

#endif // PARTOFSPEECHTAGGER_H
#include "RMSProp.h"
#include <cmath>

RMSProp::RMSProp(float learning_rate, float beta, float epsilon)
        : learning_rate(learning_rate), beta(beta), epsilon(epsilon), t(0), cache( /* initialize size appropriately */ ) {}

void RMSProp::update(
        std::vector<std::vector<float>>& weights,
        const std::vector<std::vector<float>>& weight_grads,
        std::vector<float>& biases,
        const std::vector<float>& bias_grads
) {
    t += 1;
    for (size_t i = 0; i < weights.size(); ++i) {
        for (size_t j = 0; j < weights[i].size(); ++j) {
            cache[j] = beta * cache[j] + (1 - beta) * weight_grads[i][j] * weight_grads[i][j];
            weights[i][j] -= learning_rate * weight_grads[i][j] / (std::sqrt(cache[j]) + epsilon);
        }
    }
    for (size_t i = 0; i < biases.size(); ++i) {
        biases[i] -= learning_rate * bias_grads[i];
    }
}
#include "SGD.h"

SGD::SGD(float learning_rate) : lr(learning_rate) {}

void SGD::update(std::vector<std::vector<float>>& weights,
                const std::vector<std::vector<float>>& weight_grads,
                std::vector<float>& biases,
                const std::vector<float>& bias_grads) {
    for(size_t i = 0; i < weights.size(); ++i) {
        for(size_t j = 0; j < weights[i].size(); ++j) {
            weights[i][j] -= lr * weight_grads[i][j];
        }
    }
    for(size_t i = 0; i < biases.size(); ++i) {
        biases[i] -= lr * bias_grads[i];
    }
}
#ifndef RMSPROP_H
#define RMSPROP_H

#include <vector>
#include "Optimizer.h"

class RMSProp : public Optimizer {
public:
    RMSProp(float learning_rate, float beta, float epsilon);
    void update(
            std::vector<std::vector<float>>& weights,
            const std::vector<std::vector<float>>& weight_grads,
            std::vector<float>& biases,
            const std::vector<float>& bias_grads
    ) override; // Added 'override'

private:
    float learning_rate;
    float beta;
    float epsilon;
    int t;
    std::vector<float> cache;
};

#endif // RMSPROP_H
// Adam.cpp
#include "Adam.h"
#include <cmath>
#include <stdexcept>

Adam::Adam(float lr, float b1, float b2, float eps)
        : learning_rate(lr), beta1(b1), beta2(b2), epsilon(eps), t(0) {}

void Adam::update(
        std::vector<std::vector<float>>& weights,
        const std::vector<std::vector<float>>& weight_grads,
        std::vector<float>& biases,
        const std::vector<float>& bias_grads
) {
    t += 1;

    // Initialize moment vectors if first update
    if (m_weights.empty()) {
        m_weights.resize(weights.size(), std::vector<float>(weights[0].size(), 0.0f));
        v_weights.resize(weights.size(), std::vector<float>(weights[0].size(), 0.0f));
    }
    if (m_biases.empty()) {
        m_biases.resize(biases.size(), 0.0f);
        v_biases.resize(biases.size(), 0.0f);
    }

    // Update weights
    for (size_t i = 0; i < weights.size(); ++i) {
        for (size_t j = 0; j < weights[i].size(); ++j) {
            m_weights[i][j] = beta1 * m_weights[i][j] + (1 - beta1) * weight_grads[i][j];
            v_weights[i][j] = beta2 * v_weights[i][j] + (1 - beta2) * weight_grads[i][j] * weight_grads[i][j];
            float m_hat = m_weights[i][j] / (1 - std::pow(beta1, t));
            float v_hat = v_weights[i][j] / (1 - std::pow(beta2, t));
            weights[i][j] -= learning_rate * m_hat / (std::sqrt(v_hat) + epsilon);
        }
    }

    // Update biases
    for (size_t i = 0; i < biases.size(); ++i) {
        m_biases[i] = beta1 * m_biases[i] + (1 - beta1) * bias_grads[i];
        v_biases[i] = beta2 * v_biases[i] + (1 - beta2) * bias_grads[i] * bias_grads[i];
        float m_hat = m_biases[i] / (1 - std::pow(beta1, t));
        float v_hat = v_biases[i] / (1 - std::pow(beta2, t));
        biases[i] -= learning_rate * m_hat / (std::sqrt(v_hat) + epsilon);
    }
}
#ifndef OPTIMIZER_H
#define OPTIMIZER_H

#include <vector>

class Optimizer {
public:
    virtual ~Optimizer() {}
    virtual void update(
            std::vector<std::vector<float>>& weights,
            const std::vector<std::vector<float>>& weight_grads,
            std::vector<float>& biases,
            const std::vector<float>& bias_grads
    ) = 0;
};

#endif // OPTIMIZER_H
// Adam.h
#ifndef ADAM_H
#define ADAM_H

#include <vector>
#include "Optimizer.h"

class Adam : public Optimizer {
public:
    Adam(float learning_rate, float beta1, float beta2, float epsilon);
    void update(
            std::vector<std::vector<float>>& weights,
            const std::vector<std::vector<float>>& weight_grads,
            std::vector<float>& biases,
            const std::vector<float>& bias_grads
    ) override;

private:
    float learning_rate;
    float beta1;
    float beta2;
    float epsilon;
    int t;
    std::vector<std::vector<float>> m_weights;
    std::vector<std::vector<float>> v_weights;
    std::vector<float> m_biases;
    std::vector<float> v_biases;
};

#endif // ADAM_H
#include "Optimizer.h"

class SGD : public Optimizer {
public:
    SGD(float learning_rate);
    void update(std::vector<std::vector<float>>& weights,
                const std::vector<std::vector<float>>& weight_grads,
                std::vector<float>& biases,
                const std::vector<float>& bias_grads) override;
private:
    float lr;
};
#ifndef RECURRENTLAYER_H
#define RECURRENTLAYER_H

#include <vector>

class RecurrentLayer {
public:
    RecurrentLayer(int input_size, int hidden_size);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);
private:
    int input_size;
    int hidden_size;
    std::vector<std::vector<float>> weights_input;
    std::vector<std::vector<float>> weights_hidden;
    std::vector<float> biases;
    std::vector<float> hidden_state;
};

#endif // RECURRENTLAYER_H
// DenseLayer.h
#ifndef DENSELAYER_H
#define DENSELAYER_H

#include <vector>

class DenseLayer {
public:
    DenseLayer(int input_size, int output_size);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);

    // Accessors for weights and biases
    std::vector<std::vector<float>>& get_weights();
    std::vector<float>& get_biases();

    // Accessors for gradients
    std::vector<std::vector<float>> get_weight_gradients() const;
    std::vector<float> get_bias_gradients() const;

private:
    int input_size;
    int output_size;
    std::vector<std::vector<float>> weights;
    std::vector<float> biases;
    std::vector<float> input;

    // Gradients
    std::vector<std::vector<float>> weight_gradients;
    std::vector<float> bias_gradients;
};

#endif // DENSELAYER_H
#ifndef CONVOLUTIONLAYER_H
#define CONVOLUTIONLAYER_H

#include <vector>

class ConvolutionLayer {
public:
    ConvolutionLayer(int input_channels, int output_channels, int kernel_size);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);
private:
    int input_channels;
    int output_channels;
    int kernel_size;
    std::vector<std::vector<float>> kernels;
    std::vector<float> biases;
    std::vector<float> input;
};

#endif // CONVOLUTIONLAYER_H
#include "ConvolutionLayer.h"
#include <cstdlib>

ConvolutionLayer::ConvolutionLayer(int in_ch, int out_ch, int k_size)
    : input_channels(in_ch), output_channels(out_ch), kernel_size(k_size),
      kernels(out_ch, std::vector<float>(in_ch * kernel_size * kernel_size)),
      biases(out_ch, 0.0f) {
    // Initialize kernels and biases
    for(auto &kernel : kernels) {
        for(auto &w : kernel) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
}

std::vector<float> ConvolutionLayer::forward(const std::vector<float>& input_data) {
    input = input_data;
    // Simple convolution operation (placeholder)
    std::vector<float> output(output_channels, 0.0f);
    for(int o = 0; o < output_channels; ++o) {
        for(int i = 0; i < input_channels * kernel_size * kernel_size; ++i) {
            output[o] += kernels[o][i] * input_data[i];
        }
        output[o] += biases[o];
    }
    return output;
}

std::vector<float> ConvolutionLayer::backward(const std::vector<float>& grad_output) {
    std::vector<float> grad_input(input_channels * kernel_size * kernel_size, 0.0f);
    // Simple backward operation (placeholder)
    for(int o = 0; o < output_channels; ++o) {
        for(int i = 0; i < input_channels * kernel_size * kernel_size; ++i) {
            grad_input[i] += kernels[o][i] * grad_output[o];
            // Update kernels and biases here if implementing training
        }
    }
    return grad_input;
}
// DenseLayer.cpp
#include "DenseLayer.h"
#include <cstdlib>

DenseLayer::DenseLayer(int input_sz, int output_sz)
        : input_size(input_sz), output_size(output_sz),
          weights(output_sz, std::vector<float>(input_sz)),
          biases(output_sz, 0.0f),
          weight_gradients(output_sz, std::vector<float>(input_sz, 0.0f)),
          bias_gradients(output_sz, 0.0f) {
    // Initialize weights and biases
    for(auto &row : weights) {
        for(auto &w : row) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
}

std::vector<float> DenseLayer::forward(const std::vector<float>& input_data) {
    input = input_data;
    std::vector<float> output(output_size, 0.0f);
    for(int i = 0; i < output_size; ++i) {
        for(int j = 0; j < input_size; ++j) {
            output[i] += weights[i][j] * input[j];
        }
        output[i] += biases[i];
    }
    return output;
}

std::vector<float> DenseLayer::backward(const std::vector<float>& grad_output) {
    // Compute gradients w.r.t weights and biases
    for(int i = 0; i < output_size; ++i) {
        for(int j = 0; j < input_size; ++j) {
            weight_gradients[i][j] += grad_output[i] * input[j];
        }
        bias_gradients[i] += grad_output[i];
    }

    // Compute gradient w.r.t input to propagate to previous layers
    std::vector<float> grad_input(input_size, 0.0f);
    for(int j = 0; j < input_size; ++j) {
        for(int i = 0; i < output_size; ++i) {
            grad_input[j] += weights[i][j] * grad_output[i];
        }
    }
    return grad_input;
}

std::vector<std::vector<float>>& DenseLayer::get_weights() {
    return weights;
}

std::vector<float>& DenseLayer::get_biases() {
    return biases;
}

std::vector<std::vector<float>> DenseLayer::get_weight_gradients() const {
    return weight_gradients;
}

std::vector<float> DenseLayer::get_bias_gradients() const {
    return bias_gradients;
}
#ifndef TRANSFORMERLAYER_H
#define TRANSFORMERLAYER_H

#include <vector>

class TransformerLayer {
public:
    TransformerLayer(int model_size, int num_heads);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);
private:
    int model_size;
    int num_heads;
    // Transformer-specific components
};

#endif // TRANSFORMERLAYER_H
#include "TransformerLayer.h"

TransformerLayer::TransformerLayer(int m_size, int n_heads)
    : model_size(m_size), num_heads(n_heads) {}

std::vector<float> TransformerLayer::forward(const std::vector<float>& input) {
    // Placeholder for transformer forward pass
    return input;
}

std::vector<float> TransformerLayer::backward(const std::vector<float>& grad_output) {
    // Placeholder for transformer backward pass
    return grad_output;
}
#include "RecurrentLayer.h"
#include <cstdlib>
#include <valarray>
#include <cstddef> // Add this line

RecurrentLayer::RecurrentLayer(int in_size, int hid_size)
        : input_size(in_size), hidden_size(hid_size),
          weights_input(hid_size, std::vector<float>(in_size)),
          weights_hidden(hid_size, std::vector<float>(hid_size)),
          biases(hid_size, 0.0f),
          hidden_state(hid_size, 0.0f) {
    // Initialize weights and biases
    for (auto &row : weights_input) {
        for (auto &w : row) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
    for (auto &row : weights_hidden) {
        for (auto &w : row) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
}

std::vector<float> RecurrentLayer::forward(const std::vector<float>& input_data) {
    std::vector<float> output(hidden_size, 0.0f);
    for (int h = 0; h < hidden_size; ++h) {
        for (int i = 0; i < input_size; ++i) {
            output[h] += weights_input[h][i] * input_data[i];
        }
        for (int h_prev = 0; h_prev < hidden_size; ++h_prev) {
            output[h] += weights_hidden[h][h_prev] * hidden_state[h_prev];
        }
        output[h] += biases[h];
        // Apply activation (e.g., tanh) here if needed
        hidden_state[h] = std::tanh(output[h]);
    }
    return hidden_state;
}

std::vector<float> RecurrentLayer::backward(const std::vector<float>& grad_output) {
    (void)grad_output; // Mark as intentionally unused

    // Placeholder: Implement backward logic
    return std::vector<float>();
}
#ifndef SIGMOID_H
#define SIGMOID_H

#include <vector>

class Sigmoid {
public:
    std::vector<float> activate(const std::vector<float>& input);
    std::vector<float> derivative(const std::vector<float>& input);
};

#endif // SIGMOID_H
#ifndef RELU_H
#define RELU_H

#include <vector>

class ReLU {
public:
    std::vector<float> activate(const std::vector<float>& input);
    std::vector<float> derivative(const std::vector<float>& input);
};

#endif // RELU_H
// Softmax.cpp
#include "Softmax.h"
#include <cmath>
#include <algorithm>

namespace Activation {

    std::vector<float> Softmax::derivative(const std::vector<float>& output) const {
        // Implementation of the derivative
        std::vector<float> derivative(output.size(), 0.0f);
        for (size_t i = 0; i < output.size(); ++i) {
            derivative[i] = output[i] * (1.0f - output[i]);
        }
        return derivative;
    }

    std::vector<float> Softmax::activate(const std::vector<float>& input) {
        std::vector<float> exps;
        float max_val = *std::max_element(input.begin(), input.end());
        float sum = 0.0f;
        for (auto val : input) {
            float e = std::exp(val - max_val);
            exps.push_back(e);
            sum += e;
        }
        std::vector<float> output;
        for (auto e : exps) {
            output.push_back(e / sum);
        }
        return output;
    }
} // namespace Activation
#include "Tanh.h"
#include <cmath>

std::vector<float> Tanh::activate(const std::vector<float>& input) {
    std::vector<float> output;
    for (auto val : input) {
        output.push_back(std::tanh(val));
    }
    return output;
}

std::vector<float> Tanh::derivative(const std::vector<float>& input) {
    std::vector<float> derivatives;
    for (auto val : input) {
        float tanh_val = std::tanh(val);
        derivatives.push_back(1.0f - tanh_val * tanh_val);
    }
    return derivatives;
}
#include "Sigmoid.h"
#include <cmath>

std::vector<float> Sigmoid::activate(const std::vector<float>& input) {
    std::vector<float> output;
    for (auto val : input) {
        output.push_back(1.0f / (1.0f + std::exp(-val)));
    }
    return output;
}

std::vector<float> Sigmoid::derivative(const std::vector<float>& input) {
    std::vector<float> derivatives;
    for (auto val : input) {
        float sigmoid = 1.0f / (1.0f + std::exp(-val));
        derivatives.push_back(sigmoid * (1 - sigmoid));
    }
    return derivatives;
}
#ifndef TANH_H
#define TANH_H

#include <vector>

class Tanh {
public:
    std::vector<float> activate(const std::vector<float>& input);
    std::vector<float> derivative(const std::vector<float>& input);
};

#endif // TANH_H
#ifndef SOFTMAX_H
#define SOFTMAX_H

#include <vector>

namespace Activation {

    class Softmax {
    public:
        std::vector<float> activate(const std::vector<float> &input);

        std::vector<float> derivative(const std::vector<float>& output) const;
    };

} // namespace Activation

#endif // SOFTMAX_H
#include "ReLU.h"

std::vector<float> ReLU::activate(const std::vector<float>& input) {
    std::vector<float> output;
    for (auto val : input) {
        output.push_back(val > 0 ? val : 0);
    }
    return output;
}

std::vector<float> ReLU::derivative(const std::vector<float>& input) {
    std::vector<float> derivatives;
    for (auto val : input) {
        derivatives.push_back(val > 0 ? 1.0f : 0.0f);
    }
    return derivatives;
}
// NeuralNetwork.cpp
#include "NeuralNetwork.h"
#include "Optimizers/Adam.h"
#include <cmath>

NeuralNetwork::NeuralNetwork()
        : optimizer(std::make_unique<Adam>(0.001f, 0.9f, 0.999f, 1e-8f)) { // Using Adam optimizer
    dense_layers.emplace_back(DenseLayer(100, 50));
    dense_layers.emplace_back(DenseLayer(50, 10));

    // Initialize gradient storage based on layers
    for(auto &layer : dense_layers) {
        weight_grads.emplace_back(layer.get_weight_gradients()); // Each layer's weight gradients
        bias_grads.emplace_back(layer.get_bias_gradients());     // Each layer's bias gradients
    }
}

std::vector<float> NeuralNetwork::forward(const std::vector<float>& input) {
    std::vector<float> output = input;
    output = dense_layers[0].forward(output);
    output = relu.activate(output);
    output = dense_layers[1].forward(output);
    output = softmax.activate(output);
    return output;
}

std::vector<float> NeuralNetwork::backward(const std::vector<float>& loss_grad) {
    std::vector<float> grad = softmax.derivative(loss_grad);
    grad = dense_layers[1].backward(grad);
    grad = relu.derivative(grad);
    grad = dense_layers[0].backward(grad);

    // Collect gradients from each layer
    for (std::size_t i = 0; i < dense_layers.size(); ++i) {
        auto layer_weight_grads = dense_layers[i].get_weight_gradients();
        auto layer_bias_grads = dense_layers[i].get_bias_gradients();
        weight_grads[i] = layer_weight_grads;
        bias_grads[i] = layer_bias_grads;
    }

    return grad;
}


void NeuralNetwork::update_parameters() {
    // Collect all weights and biases and their gradients
    std::vector<std::vector<float>> all_weights;
    std::vector<std::vector<float>> all_weight_grads;
    std::vector<float> all_biases;
    std::vector<float> all_bias_grads;

    for (std::size_t i = 0; i < dense_layers.size(); ++i) {
        auto &w = dense_layers[i].get_weights(); // std::vector<std::vector<float>>
        auto &b = dense_layers[i].get_biases();  // std::vector<float>
        auto &wg = weight_grads[i];              // std::vector<std::vector<float>>
        auto &bg = bias_grads[i];                // std::vector<float>

        // Flatten weights and weight gradients
        for(auto &weight_vec : w) {
            all_weights.emplace_back(weight_vec);
        }
        for(auto &weight_grad_vec : wg) {
            all_weight_grads.emplace_back(weight_grad_vec);
        }

        // Flatten biases and bias gradients
        for(auto &bias : b) {
            all_biases.emplace_back(bias);
        }
        for(auto &bias_grad : bg) {
            all_bias_grads.emplace_back(bias_grad);
        }
    }

    // Update using optimizer
    optimizer->update(all_weights, all_weight_grads, all_biases, all_bias_grads);

    // Distribute updated weights and biases back to layers
    size_t idx = 0;
    for(auto &layer : dense_layers) {
        auto &w = layer.get_weights();
        auto &b = layer.get_biases();
        // Assign weights
        for(auto &weight_vec : w) {
            weight_vec = all_weights[idx++];
        }
        // Assign biases
        for(auto &bias : b) {
            bias = all_biases[idx++];
        }
    }
}
// NeuralNetwork.h
#ifndef NEURALNETWORK_H
#define NEURALNETWORK_H

#include <vector>
#include <memory>
#include "Layers/DenseLayer.h"
#include "Layers/ConvolutionLayer.h"
#include "Layers/RecurrentLayer.h"
#include "Layers/TransformerLayer.h"
#include "Activation/ReLU.h"
#include "Activation/Sigmoid.h"
#include "Activation/Softmax.h"
#include "Activation/Tanh.h"
#include "Optimizers/Optimizer.h"

class NeuralNetwork {
public:
    NeuralNetwork();
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& loss_grad);
    void update_parameters();
private:
    std::vector<DenseLayer> dense_layers;
    ReLU relu;
    Sigmoid sigmoid;
    Activation::Softmax softmax;
    Tanh tanh;
    std::unique_ptr<Optimizer> optimizer;

    // Corrected Gradient Storage
    std::vector<std::vector<std::vector<float>>> weight_grads; // [layer][neuron][weight]
    std::vector<std::vector<float>> bias_grads;               // [layer][bias]
};

#endif // NEURALNETWORK_H
// Logger.cpp
#include "Logger.h"
#include <iostream>
#include <fstream>
#include <ctime>

Logger::Logger(LogLevel level) : current_level(level) {}

void Logger::log(const std::string& message, LogLevel level) {
    if(level < current_level) return;

    std::time_t now = std::time(nullptr);
    char buf[100];
    std::strftime(buf, sizeof(buf), "%Y-%m-%d %H:%M:%S", std::localtime(&now));
    std::string log_message = std::string(buf) + " - " + message;

    switch(level) {
        case INFO:
            std::cout << "[INFO] " << log_message << std::endl;
            break;
        case DEBUG:
            std::cout << "[DEBUG] " << log_message << std::endl;
            break;
        case ERROR:
            std::cerr << "[ERROR] " << log_message << std::endl;
            break;
    }

    // Optionally, write to a log file
}
// Config.cpp
#include "Config.h"
#include <fstream>
#include <stdexcept>

Config::Config(const std::string& config_path) {
    std::ifstream infile(config_path);
    if (!infile.is_open()) {
        throw std::runtime_error("Failed to open config file.");
    }
    infile >> config_json;
    infile.close();
}

template<typename T>
T Config::get(const std::string& key) const {
    if(config_json.contains(key)) {
        return config_json[key].get<T>();
    }
    throw std::runtime_error("Key not found in config.");
}

// Explicit template instantiation
template int Config::get<int>(const std::string&) const;
template float Config::get<float>(const std::string&) const;
template std::string Config::get<std::string>(const std::string&) const;
// Add more as needed
#include "FileUtils.h"
#include <sys/stat.h>

bool FileUtils::file_exists(const std::string& path) {
    struct stat buffer;
    return (stat (path.c_str(), &buffer) == 0);
}
// Config.h
#ifndef CONFIG_H
#define CONFIG_H

#include <string>
#include <nlohmann/json.hpp>

class Config {
public:
    Config(const std::string& config_path);

    template<typename T>
    T get(const std::string& key) const;

private:
    nlohmann::json config_json;
};

#endif // CONFIG_H
// Logger.h
#ifndef LOGGER_H
#define LOGGER_H

#include <string>

enum LogLevel {
    INFO,
    DEBUG,
    ERROR
};

class Logger {
public:
    Logger(LogLevel level = INFO);
    void log(const std::string& message, LogLevel level = INFO);
private:
    LogLevel current_level;
};

#endif // LOGGER_H
#ifndef FILEUTILS_H
#define FILEUTILS_H

#include <string>

class FileUtils {
public:
    static bool file_exists(const std::string& path);
};

#endif // FILEUTILS_H
#include "interface/InterfaceEngine.h"
#include <pybind11/embed.h>

namespace py = pybind11;

int main() {
    py::scoped_interpreter guard{};
    py::module::import("InterfaceEngine"); // Correct module name
    return 0;
}
