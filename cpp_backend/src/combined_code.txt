#include <pybind11/pybind11.h>
#include <pybind11/stl.h>  // Enable automatic conversion for STL containers
#include "InterfaceEngine.h"
#include "data/DataLoader.h"

namespace py = pybind11;

PYBIND11_MODULE(InterfaceEngine, m) {
    m.doc() = "InterfaceEngine Python bindings";

    // Register custom exceptions
    py::register_exception<std::invalid_argument>(m, "InvalidArgumentError");
    py::register_exception<std::runtime_error>(m, "RuntimeError");

    // Expose DataLoader
    py::class_<DataLoader>(m, "DataLoader")
        .def(py::init<const std::string&>(), py::arg("data_path"))
        .def("load_data", &DataLoader::load_data);

    // Expose InterfaceEngine
    py::class_<InterfaceEngine>(m, "InterfaceEngine")
        .def(py::init<const std::string&, const std::string&>(),
             py::arg("data_path"),
             py::arg("model_output_path"))
        .def("predict", &InterfaceEngine::predict)
        .def("train", &InterfaceEngine::train);
}
#ifndef INTERFACEENGINE_H
#define INTERFACEENGINE_H

#include <string>
#include <vector>
#include <memory>
#include "../model/NeuralNetwork.h"
#include "../nlp/NLPProcessor.h"
#include "../utils/Logger.h"
#include "../training/LossFunctions/LossFunction.h"
#include "../training/Metrics/Metric.h"
#include "../data/DataLoader.h"
#include "../utils/Config.h"
#include "../training/Trainer.h"

class InterfaceEngine {
public:
    InterfaceEngine(const std::string& data_path, const std::string& model_output_path);
    std::vector<float> predict(const std::string& input_text);
    void train(int epochs, size_t batch_size);

    // Save and load model
    void save_model(const std::string& path);
    void load_model(const std::string& path);

private:
    NeuralNetwork network;
    NLPProcessor nlp_processor;
    Logger logger;

    // Training data
    std::vector<std::string> training_data;

    // Loss function and metric
    std::unique_ptr<LossFunction> loss_fn;
    std::unique_ptr<Metric> metric_fn;

    // Configuration
    Config config;

    // Helper functions
    std::vector<float> get_target_for_data(const std::string& data);
    float calculate_loss(const std::vector<float>& predictions, const std::vector<float>& targets);
    std::vector<float> calculate_gradient(const std::vector<float>& predictions, const std::vector<float>& targets);
};

#endif // INTERFACEENGINE_H
#include "InterfaceEngine.h"
#include "../training/LossFunctions/CrossEntropyLoss.h"
#include "../training/Metrics/Accuracy.h"
#include "../data/Preprocessing/TextCleaner.h"
#include "../data/Tokenizer.h"
#include "../utils/FileUtils.h"
#include <algorithm> // For std::min
#include <random>

InterfaceEngine::InterfaceEngine(const std::string& data_path, const std::string& model_output_path)
    : network(), nlp_processor(), logger(INFO), config("../config/config.json") {
    // Initialize with data and model paths
    DataLoader data_loader(data_path);
    training_data = data_loader.load_data();

    // Initialize loss function and metric
    loss_fn = std::make_unique<CrossEntropyLoss>();
    metric_fn = std::make_unique<Accuracy>();

    // Load model if exists
    if (FileUtils::file_exists(model_output_path + "model.bin")) {
        load_model(model_output_path + "model.bin");
        logger.log("Model loaded from " + model_output_path + "model.bin");
    } else {
        logger.log("No existing model found. Starting with a new model.");
    }
}

std::vector<float> InterfaceEngine::predict(const std::string& input_text) {
    std::vector<std::string> tokens = nlp_processor.process_text(input_text);

    // Fixed input size
    const size_t fixed_input_size = 100;
    std::vector<float> input_vector(fixed_input_size, 0.0f);

    // Convert tokens to input vector
    for (size_t i = 0; i < std::min(tokens.size(), fixed_input_size); ++i) {
        // Implement embedding lookup or token encoding
        input_vector[i] = nlp_processor.get_token_id(tokens[i]);
    }

    return network.forward(input_vector);
}

void InterfaceEngine::train(int epochs, size_t batch_size) {
    const size_t fixed_input_size = 100; // Ensure consistent input size
    size_t total_batches = (training_data.size() + batch_size - 1) / batch_size;

    for (int epoch = 0; epoch < epochs; ++epoch) {
        logger.log("Starting epoch " + std::to_string(epoch + 1));

        // Shuffle training data to improve training
        std::random_device rd;
        std::mt19937 g(rd());
        std::shuffle(training_data.begin(), training_data.end(), g);

        float epoch_loss = 0.0f;
        float epoch_accuracy = 0.0f;

        for (size_t batch_num = 0; batch_num < total_batches; ++batch_num) {
            size_t start_idx = batch_num * batch_size;
            size_t end_idx = std::min(start_idx + batch_size, training_data.size());

            // Vectors to accumulate gradients over the batch
            float batch_loss = 0.0f;
            float batch_accuracy = 0.0f;

            for (size_t idx = start_idx; idx < end_idx; ++idx) {
                const auto& data = training_data[idx];
                std::vector<std::string> tokens = nlp_processor.process_text(data);

                // Prepare input vector with fixed size
                std::vector<float> input_vector(fixed_input_size, 0.0f);
                for (size_t i = 0; i < std::min(tokens.size(), fixed_input_size); ++i) {
                    input_vector[i] = nlp_processor.get_token_id(tokens[i]);
                }

                // Forward pass
                std::vector<float> output = network.forward(input_vector);

                // Get target and compute loss
                std::vector<float> target = get_target_for_data(data);
                float loss = loss_fn->compute_loss(output, target);
                batch_loss += loss;

                // Compute gradient and backpropagate
                std::vector<float> grad_output = loss_fn->compute_gradient(output, target);
                network.backward(grad_output);

                // Update accuracy
                batch_accuracy += metric_fn->compute(output, target);
            }

            // Update network parameters after processing the batch
            network.update_parameters();

            // Log batch loss and accuracy
            logger.log("Epoch " + std::to_string(epoch + 1) + ", Batch " + std::to_string(batch_num + 1) +
                       "/" + std::to_string(total_batches) + ": Loss = " + 
                       std::to_string(batch_loss / (end_idx - start_idx)) +
                       ", Accuracy = " + std::to_string(batch_accuracy / (end_idx - start_idx)));

            epoch_loss += batch_loss;
            epoch_accuracy += batch_accuracy;
        }

        // Log epoch metrics
        logger.log("Epoch " + std::to_string(epoch + 1) + " completed. Average Loss: " +
                   std::to_string(epoch_loss / training_data.size()) +
                   ", Average Accuracy: " + std::to_string(epoch_accuracy / training_data.size()));
    }

    // Save the trained model
    save_model("../Model/model.bin");
    logger.log("Model saved after training.");
}

std::vector<float> InterfaceEngine::get_target_for_data(const std::string& data) {
    // Implement logic to convert data to target vectors
    // For this example, we will generate a dummy target vector
    std::vector<float> target(10, 0.0f); // Assuming 10 classes
    int random_class = std::hash<std::string>{}(data) % 10;
    target[random_class] = 1.0f;
    return target;
}

float InterfaceEngine::calculate_loss(const std::vector<float>& predictions, const std::vector<float>& targets) {
    return loss_fn->compute_loss(predictions, targets);
}

std::vector<float> InterfaceEngine::calculate_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) {
    return loss_fn->compute_gradient(predictions, targets);
}

void InterfaceEngine::save_model(const std::string& path) {
    // Implement model saving logic
    network.save(path);
}

void InterfaceEngine::load_model(const std::string& path) {
    // Implement model loading logic
    network.load(path);
}
#ifndef CROSSENTROPYLOSS_H
#define CROSSENTROPYLOSS_H

#include "LossFunction.h"
#include <vector>

class CrossEntropyLoss : public LossFunction {
public:
    float compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) override;
    std::vector<float> compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // CROSSENTROPYLOSS_H
//
// Created by Rahat on 09/11/2024.
//

#include "LossFunction.h"
#ifndef MEANSQUAREDLOSS_H
#define MEANSQUAREDLOSS_H

#include "LossFunction.h"

class MeanSquaredLoss : public LossFunction {
public:
    float compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) override;
    std::vector<float> compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // MEANSQUAREDLOSS_H
#include "CrossEntropyLoss.h"
#include <cmath>
#include <algorithm>

float CrossEntropyLoss::compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) {
    float loss = 0.0f;
    for(size_t i = 0; i < predictions.size(); ++i) {
        float pred = std::max(predictions[i], 1e-15f);
        loss -= targets[i] * std::log(pred);
    }
    return loss;
}

std::vector<float> CrossEntropyLoss::compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) {
    std::vector<float> grad(predictions.size(), 0.0f);
    for(size_t i = 0; i < predictions.size(); ++i) {
        float pred = std::max(predictions[i], 1e-15f);
        grad[i] = -targets[i] / pred;
    }
    return grad;
}
#include <cstddef> // Add this line
#include "MeanSquaredLoss.h"
#include <cmath>

float MeanSquaredLoss::compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) {
    float loss = 0.0f;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        float diff = predictions[i] - targets[i];
        loss += diff * diff;
    }
    return loss / predictions.size();
}

std::vector<float> MeanSquaredLoss::compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) {
    std::vector<float> grad(predictions.size(), 0.0f);
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        grad[i] = 2.0f * (predictions[i] - targets[i]) / predictions.size();
    }
    return grad;
}
#ifndef LOSSFUNCTION_H
#define LOSSFUNCTION_H

#include <vector>

class LossFunction {
public:
    virtual ~LossFunction() {}
    virtual float compute_loss(const std::vector<float>& predictions, const std::vector<float>& targets) = 0;
    virtual std::vector<float> compute_gradient(const std::vector<float>& predictions, const std::vector<float>& targets) = 0;
};

#endif // LOSSFUNCTION_H
#include <cstddef> // Add this line
#include "Recall.h"

float Recall::compute(const std::vector<float>& predictions, const std::vector<float>& targets) {
    int true_positive = 0;
    int actual_positive = 0;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        if (targets[i] > 0.5f) {
            actual_positive++;
            if (predictions[i] > 0.5f) {
                true_positive++;
            }
        }
    }
    return actual_positive > 0 ? static_cast<float>(true_positive) / actual_positive : 0.0f;
}
#include "Accuracy.h"
#include <algorithm>

float Accuracy::compute(const std::vector<float>& predictions, const std::vector<float>& targets) {
    auto pred_it = std::max_element(predictions.begin(), predictions.end());
    auto target_it = std::max_element(targets.begin(), targets.end());
    int pred_class = std::distance(predictions.begin(), pred_it);
    int target_class = std::distance(targets.begin(), target_it);
    return pred_class == target_class ? 1.0f : 0.0f;
}
#ifndef METRIC_H
#define METRIC_H

#include <vector>

class Metric {
public:
    virtual ~Metric() {}
    virtual float compute(const std::vector<float>& predictions, const std::vector<float>& targets) = 0;
};

#endif // METRIC_H
//
// Created by Rahat on 09/11/2024.
//

#include "Metric.h"
#include <cstddef> // Add this line
#include "Precision.h"

float Precision::compute(const std::vector<float>& predictions, const std::vector<float>& targets) {
    int true_positive = 0;
    int predicted_positive = 0;
    for (std::size_t i = 0; i < predictions.size(); ++i) {
        if (predictions[i] > 0.5f) {
            predicted_positive++;
            if (targets[i] > 0.5f) {
                true_positive++;
            }
        }
    }
    return predicted_positive > 0 ? static_cast<float>(true_positive) / predicted_positive : 0.0f;
}
#ifndef RECALL_H
#define RECALL_H

#include "Metric.h"

class Recall : public Metric {
public:
    float compute(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // RECALL_H
#ifndef ACCURACY_H
#define ACCURACY_H

#include "Metric.h"

class Accuracy : public Metric {
public:
    float compute(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // ACCURACY_H
#ifndef PRECISION_H
#define PRECISION_H

#include "Metric.h"

class Precision : public Metric {
public:
    float compute(const std::vector<float>& predictions, const std::vector<float>& targets) override;
};

#endif // PRECISION_H
#ifndef TRAINER_H
#define TRAINER_H

#include "../model/NeuralNetwork.h"
#include "LossFunctions/LossFunction.h"
#include "LossFunctions/CrossEntropyLoss.h"
#include "LossFunctions/MeanSquaredLoss.h"
#include "Metrics/Metric.h"
#include "Metrics/Accuracy.h"
#include "../data/DataLoader.h"
#include "../nlp/NLPProcessor.h"
#include <vector>
#include <string>

class Trainer {
public:
    Trainer(NeuralNetwork& network, LossFunction& loss_fn, Metric& metric, DataLoader& data_loader, NLPProcessor& nlp_processor);
    void train(int epochs);
private:
    NeuralNetwork& net;
    LossFunction& loss;
    Metric& metric_fn;
    DataLoader& loader;
    NLPProcessor& nlp;
};

#endif // TRAINER_H
#include "Trainer.h"
#include "../utils/Logger.h"
#include "../nlp/NLPProcessor.h"

Trainer::Trainer(NeuralNetwork& network, LossFunction& loss_fn, Metric& metric, DataLoader& data_loader, NLPProcessor& nlp_processor)
    : net(network), loss(loss_fn), metric_fn(metric), loader(data_loader), nlp(nlp_processor) {}

void Trainer::train(int epochs) {
    Logger logger(INFO);
    for(int epoch = 1; epoch <= epochs; ++epoch) {
        float epoch_loss = 0.0f;
        float epoch_metric = 0.0f;
        std::vector<std::string> data = loader.load_data();
        for(const auto &text : data) {
            std::vector<std::string> tokens = nlp.process_text(text);
            // Placeholder: Convert tokens to input vector
            std::vector<float> input(100, 1.0f); // Example input
            std::vector<float> targets(10, 0.0f); // Example target
            input = net.forward(input);
            float current_loss = loss.compute_loss(input, targets);
            epoch_loss += current_loss;
            std::vector<float> grad = loss.compute_gradient(input, targets);
            net.backward(grad);
            net.update_parameters();
            epoch_metric += metric_fn.compute(input, targets);
        }
        logger.log("Epoch " + std::to_string(epoch) + " Loss: " + std::to_string(epoch_loss / data.size()) + " Metric: " + std::to_string(epoch_metric / data.size()));
    }
}
#include "EmbeddingLoader.h"
#include <fstream>
#include <sstream>
#include <stdexcept>

EmbeddingLoader::EmbeddingLoader(const std::string& embedding_path) {
    load_embeddings(embedding_path);
}

void EmbeddingLoader::load_embeddings(const std::string& path) {
    std::ifstream infile(path);
    if (!infile.is_open()) {
        throw std::runtime_error("Failed to open embedding file.");
    }
    std::string line;
    while (std::getline(infile, line)) {
        std::istringstream iss(line);
        std::string word;
        iss >> word;
        std::vector<float> vec;
        float val;
        while (iss >> val) {
            vec.push_back(val);
        }
        embeddings[word] = vec;
    }
    infile.close();
}

std::vector<float> EmbeddingLoader::get_embedding(const std::string& word) {
    if (embeddings.find(word) != embeddings.end()) {
        return embeddings[word];
    }
    return std::vector<float>();
}
#ifndef WORDEMBEDDINGS_H
#define WORDEMBEDDINGS_H

#include "EmbeddingLoader.h"
#include <vector>
#include <string>

class WordEmbeddings {
public:
    WordEmbeddings(const std::string& embedding_path);
    std::vector<float> get_word_embedding(const std::string& word);
private:
    EmbeddingLoader loader;
};

#endif // WORDEMBEDDINGS_H
#include "WordEmbeddings.h"

WordEmbeddings::WordEmbeddings(const std::string& embedding_path)
    : loader(embedding_path) {}

std::vector<float> WordEmbeddings::get_word_embedding(const std::string& word) {
    return loader.get_embedding(word);
}
#ifndef EMBEDDINGLOADER_H
#define EMBEDDINGLOADER_H

#include <string>
#include <unordered_map>
#include <vector>

class EmbeddingLoader {
public:
    EmbeddingLoader(const std::string& embedding_path);
    std::vector<float> get_embedding(const std::string& word);
private:
    std::unordered_map<std::string, std::vector<float>> embeddings;
    void load_embeddings(const std::string& path);
};

#endif // EMBEDDINGLOADER_H
#ifndef TEXTCLEANER_H
#define TEXTCLEANER_H

#include <string>

class TextCleaner {
public:
    TextCleaner();
    std::string clean_text(const std::string& text);
private:
    void to_lowercase(std::string& text);
    void remove_punctuation(std::string& text);
};

#endif // TEXTCLEANER_H
// Lemmatiser.cpp
#include "Lemmatiser.h"
#include <algorithm>

Lemmatiser::Lemmatiser() {
    // Initialize lemmatizer with a simple dictionary
    lemma_dict = {
            {"running", "run"},
            {"jumps", "jump"},
            {"easily", "easy"},
            // Add more word-lemma pairs as needed
    };
}

std::string Lemmatiser::lemmatize(const std::string& word) const {
    std::string lower_word = word;
    std::transform(lower_word.begin(), lower_word.end(), lower_word.begin(), ::tolower);

    auto it = lemma_dict.find(lower_word);
    if (it != lemma_dict.end()) {
        return it->second;
    }
    return lower_word;
}

void Lemmatiser::addLemma(const std::string& word, const std::string& lemma) {
    std::string lower_word = word;
    std::transform(lower_word.begin(), lower_word.end(), lower_word.begin(),
                   [](unsigned char c) { return std::tolower(c); });
    lemma_dict[lower_word] = lemma;
}
#include "TextCleaner.h"
#include <algorithm>
#include <cctype>

TextCleaner::TextCleaner() {}

void TextCleaner::to_lowercase(std::string& text) {
    std::transform(text.begin(), text.end(), text.begin(),
                   [](unsigned char c){ return std::tolower(c); });
}

void TextCleaner::remove_punctuation(std::string& text) {
    text.erase(std::remove_if(text.begin(), text.end(),
        [](unsigned char c) { return std::ispunct(c); }), text.end());
}

std::string TextCleaner::clean_text(const std::string& text) {
    std::string cleaned = text;
    to_lowercase(cleaned);
    remove_punctuation(cleaned);
    return cleaned;
}
// Lemmatiser.h
#ifndef LEMMATISER_H
#define LEMMATISER_H

#include <string>
#include <unordered_map>

class Lemmatiser {
public:
    Lemmatiser();
    std::string lemmatize(const std::string& word) const;
    void addLemma(const std::string& word, const std::string& lemma);

private:
    std::unordered_map<std::string, std::string> lemma_dict;
};

#endif // LEMMATISER_H
#ifndef STOPWORDSREMOVER_H
#define STOPWORDSREMOVER_H

#include <string>
#include <vector>
#include <unordered_set>

class StopWordsRemover {
public:
    StopWordsRemover();
    std::vector<std::string> remove_stopwords(const std::vector<std::string>& words);
private:
    std::unordered_set<std::string> stopwords;
    void load_stopwords();
};

#endif // STOPWORDSREMOVER_H
#include "StopWordsRemover.h"

StopWordsRemover::StopWordsRemover() {
    load_stopwords();
}

void StopWordsRemover::load_stopwords() {
    stopwords = {"a", "an", "the", "and", "or", "but", "if", "while", "with", "to", "of"};
}

std::vector<std::string> StopWordsRemover::remove_stopwords(const std::vector<std::string>& words) {
    std::vector<std::string> filtered;
    for (const auto& word : words) {
        if (stopwords.find(word) == stopwords.end()) {
            filtered.push_back(word);
        }
    }
    return filtered;
}
#include "Tokenizer.h"
#include <sstream>

Tokenizer::Tokenizer() {}

std::vector<std::string> Tokenizer::tokenize(const std::string& text) {
    std::vector<std::string> tokens;
    std::istringstream iss(text);
    std::string token;
    while (iss >> token) {
        tokens.push_back(token);
    }
    return tokens;
}
#ifndef DATALOADER_H
#define DATALOADER_H

#include <string>
#include <vector>

class DataLoader {
public:
    DataLoader(const std::string& data_path);
    std::vector<std::string> load_data(); // Changed from vector<unordered_map>
private:
    std::string data_path;
};

#endif // DATALOADER_H
#include "DataLoader.h"
#include <arrow/api.h>
#include <arrow/io/api.h>
#include <parquet/arrow/reader.h>
#include <stdexcept>
#include <iostream>

DataLoader::DataLoader(const std::string& data_path) : data_path(data_path) {}

std::vector<std::string> DataLoader::load_data() {
    try {
        // Initialize Arrow's memory pool
        arrow::MemoryPool* pool = arrow::default_memory_pool();

        // Open the Parquet file
        std::shared_ptr<arrow::io::ReadableFile> infile;
        auto open_result = arrow::io::ReadableFile::Open(data_path, pool);
        if (!open_result.ok()) {
            std::cerr << "Failed to open data file at: " << data_path 
                      << "\nError: " << open_result.status().ToString() << std::endl;
            throw std::runtime_error("Failed to open data file.");
        }
        infile = *open_result;

        // Create Parquet reader
        std::unique_ptr<parquet::arrow::FileReader> reader;
        arrow::Status reader_result = parquet::arrow::OpenFile(infile, pool, &reader);
        if (!reader_result.ok()) {
            std::cerr << "Failed to create Parquet reader.\nError: " << reader_result.ToString() << std::endl;
            throw std::runtime_error("Failed to create Parquet reader.");
        }

        // Read the entire file into a table
        std::shared_ptr<arrow::Table> table;
        arrow::Status table_result = reader->ReadTable(&table);
        if (!table_result.ok()) {
            std::cerr << "Failed to read Parquet table.\nError: " << table_result.ToString() << std::endl;
            throw std::runtime_error("Failed to read Parquet table.");
        }

        // Extract data from the table
        std::vector<std::string> data;

        auto schema = table->schema();
        int num_columns = schema->num_fields();
        std::vector<std::string> column_names;
        for (int i = 0; i < num_columns; ++i) {
            column_names.emplace_back(schema->field(i)->name());
        }

        // Specify the target column
        std::string target_column = "markdown"; // Updated to match the Parquet schema
        int target_col_index = -1;
        for (int i = 0; i < num_columns; ++i) {
            if (column_names[i] == target_column) {
                target_col_index = i;
                break;
            }
        }

        if (target_col_index == -1) {
            std::cerr << "DataLoader Error: Target column '" << target_column << "' not found in Parquet file." << std::endl;
            std::cerr << "Available columns are:" << std::endl;
            for (const auto& col : column_names) {
                std::cerr << " - " << col << std::endl;
            }
            throw std::runtime_error("Target column not found in Parquet file.");
        }

        // Iterate through all chunks
        int num_chunks = table->column(target_col_index)->num_chunks();
        for (int chunk_index = 0; chunk_index < num_chunks; ++chunk_index) {
            auto chunk = table->column(target_col_index)->chunk(chunk_index);
            auto string_array = std::static_pointer_cast<arrow::StringArray>(chunk);

            int num_rows = string_array->length();
            for (int row = 0; row < num_rows; ++row) {
                if (!string_array->IsNull(row)) {
                    data.emplace_back(string_array->GetString(row));
                } else {
                    data.emplace_back("");
                }
            }
        }

        std::cout << "DataLoader: Successfully loaded " << data.size() << " entries." << std::endl;
        return data;
    }
    catch (const std::bad_alloc& e) {
        std::cerr << "DataLoader Bad Allocation Error: " << e.what() << std::endl;
        throw;
    }
    catch (const std::exception& e) {
        std::cerr << "DataLoader Exception: " << e.what() << std::endl;
        throw;
    }
}
#ifndef TOKENIZER_H
#define TOKENIZER_H

#include <string>
#include <vector>

class Tokenizer {
public:
    Tokenizer();
    std::vector<std::string> tokenize(const std::string& text);
};

#endif // TOKENIZER_H
#ifndef SENTIMENTANALYSIS_H
#define SENTIMENTANALYSIS_H

#include <vector>
#include <string>

class SentimentAnalysis {
public:
    SentimentAnalysis();
    float analyze(const std::vector<std::string>& tokens);
};

#endif // SENTIMENTANALYSIS_H
#ifndef NAMEDENTITYRECOGNITION_H
#define NAMEDENTITYRECOGNITION_H

#include <vector>
#include <string>

class NamedEntityRecognition {
public:
    NamedEntityRecognition();
    std::vector<std::pair<std::string, std::string>> recognize(const std::vector<std::string>& tokens);
};

#endif // NAMEDENTITYRECOGNITION_H
#include "SentimentAnalysis.h"

SentimentAnalysis::SentimentAnalysis() {}

float SentimentAnalysis::analyze(const std::vector<std::string>& tokens) {
    (void)tokens; // Mark as intentionally unused

    // Placeholder: Implement sentiment analysis logic
    return 0.0f;
}
#include "PartOfSpeechTagger.h"

std::vector<std::pair<std::string, std::string>> PartOfSpeechTagger::tag(const std::vector<std::string>& tokens) {
    (void)tokens; // Mark as intentionally unused

    // Placeholder: Implement POS tagging logic
    return std::vector<std::pair<std::string, std::string>>();
}
#include "NamedEntityRecognition.h"

std::vector<std::pair<std::string, std::string>> NamedEntityRecognition::recognize(const std::vector<std::string>& tokens) {
    (void)tokens; // Mark as intentionally unused

    // Placeholder: Implement NER logic
    return std::vector<std::pair<std::string, std::string>>();
}
#include "NLPProcessor.h"
#include <fstream>
#include <sstream>

NLPProcessor::NLPProcessor() : config("../config/config.json") {
    build_vocabulary();
}

void NLPProcessor::build_vocabulary() {
    // Load vocabulary from file or build from dataset
    std::string vocab_path = config.get<std::string>("vocabulary_path");
    std::ifstream infile(vocab_path);
    if (!infile.is_open()) {
        // Handle error or build vocabulary from data
        return;
    }
    std::string line;
    int index = 0;
    while (std::getline(infile, line)) {
        vocabulary[line] = index++;
    }
    infile.close();
}

std::vector<std::string> NLPProcessor::process_text(const std::string& text) {
    std::string cleaned = cleaner.clean_text(text);
    std::vector<std::string> tokens = tokenizer.tokenize(cleaned);
    tokens = stopwords_remover.remove_stopwords(tokens);
    for(auto &token : tokens) {
        token = lemmatiser.lemmatize(token);
    }
    return tokens;
}

int NLPProcessor::get_token_id(const std::string& token) {
    auto it = vocabulary.find(token);
    if (it != vocabulary.end()) {
        return it->second;
    } else {
        // Handle unknown token
        return vocabulary.size(); // Return a special index for unknown tokens
    }
}
#ifndef NLPPROCESSOR_H
#define NLPPROCESSOR_H

#include <string>
#include <vector>
#include <unordered_map>
#include "../data/Preprocessing/TextCleaner.h"
#include "../data/Preprocessing/StopWordsRemover.h"
#include "../data/Preprocessing/Lemmatiser.h"
#include "../data/Tokenizer.h"
#include "../utils/Config.h"

class NLPProcessor {
public:
    NLPProcessor();
    std::vector<std::string> process_text(const std::string& text);
    int get_token_id(const std::string& token);

private:
    TextCleaner cleaner;
    StopWordsRemover stopwords_remover;
    Lemmatiser lemmatiser;
    Tokenizer tokenizer;
    std::unordered_map<std::string, int> vocabulary;
    Config config;

    void build_vocabulary();
};

#endif // NLPPROCESSOR_H
#ifndef PARTOFSPEECHTAGGER_H
#define PARTOFSPEECHTAGGER_H

#include <vector>
#include <string>

class PartOfSpeechTagger {
public:
    PartOfSpeechTagger();
    std::vector<std::pair<std::string, std::string>> tag(const std::vector<std::string>& tokens);
};

#endif // PARTOFSPEECHTAGGER_H
#include "RMSProp.h"
#include <cmath>

RMSProp::RMSProp(float learning_rate, float beta, float epsilon)
        : learning_rate(learning_rate), beta(beta), epsilon(epsilon), t(0), cache( /* initialize size appropriately */ ) {}

void RMSProp::update(
        std::vector<std::vector<float>>& weights,
        const std::vector<std::vector<float>>& weight_grads,
        std::vector<float>& biases,
        const std::vector<float>& bias_grads
) {
    t += 1;
    for (size_t i = 0; i < weights.size(); ++i) {
        for (size_t j = 0; j < weights[i].size(); ++j) {
            cache[j] = beta * cache[j] + (1 - beta) * weight_grads[i][j] * weight_grads[i][j];
            weights[i][j] -= learning_rate * weight_grads[i][j] / (std::sqrt(cache[j]) + epsilon);
        }
    }
    for (size_t i = 0; i < biases.size(); ++i) {
        biases[i] -= learning_rate * bias_grads[i];
    }
}
#include "SGD.h"

SGD::SGD(float learning_rate) : lr(learning_rate) {}

void SGD::update(std::vector<std::vector<float>>& weights,
                const std::vector<std::vector<float>>& weight_grads,
                std::vector<float>& biases,
                const std::vector<float>& bias_grads) {
    for(size_t i = 0; i < weights.size(); ++i) {
        for(size_t j = 0; j < weights[i].size(); ++j) {
            weights[i][j] -= lr * weight_grads[i][j];
        }
    }
    for(size_t i = 0; i < biases.size(); ++i) {
        biases[i] -= lr * bias_grads[i];
    }
}
#ifndef RMSPROP_H
#define RMSPROP_H

#include <vector>
#include "Optimizer.h"

class RMSProp : public Optimizer {
public:
    RMSProp(float learning_rate, float beta, float epsilon);
    void update(
            std::vector<std::vector<float>>& weights,
            const std::vector<std::vector<float>>& weight_grads,
            std::vector<float>& biases,
            const std::vector<float>& bias_grads
    ) override; // Added 'override'

private:
    float learning_rate;
    float beta;
    float epsilon;
    int t;
    std::vector<float> cache;
};

#endif // RMSPROP_H
#include "Adam.h"
#include <cmath>

Adam::Adam(float lr, float b1, float b2, float eps)
        : learning_rate(lr), beta1(b1), beta2(b2), epsilon(eps), t(0) {}

void Adam::update(
    std::vector<std::vector<float>>& weights,
    const std::vector<std::vector<float>>& weight_grads,
    std::vector<float>& biases,
    const std::vector<float>& bias_grads
) {
    t += 1;

    // Initialize moment vectors if first update
    if (m_weights.empty()) {
        m_weights.resize(weights.size());
        v_weights.resize(weights.size());
        for (size_t i = 0; i < weights.size(); ++i) {
            m_weights[i].resize(weights[i].size(), 0.0f);
            v_weights[i].resize(weights[i].size(), 0.0f);
        }
    }

    if (m_biases.empty()) {
        m_biases.resize(biases.size(), 0.0f);
        v_biases.resize(biases.size(), 0.0f);
    }

    // Update weights
    for (size_t i = 0; i < weights.size(); ++i) {
        for (size_t j = 0; j < weights[i].size(); ++j) {
            m_weights[i][j] = beta1 * m_weights[i][j] + (1 - beta1) * weight_grads[i][j];
            v_weights[i][j] = beta2 * v_weights[i][j] + (1 - beta2) * weight_grads[i][j] * weight_grads[i][j];
            float m_hat = m_weights[i][j] / (1 - std::pow(beta1, t));
            float v_hat = v_weights[i][j] / (1 - std::pow(beta2, t));
            weights[i][j] -= learning_rate * m_hat / (std::sqrt(v_hat) + epsilon);
        }
    }

    // Update biases
    for (size_t i = 0; i < biases.size(); ++i) {
        m_biases[i] = beta1 * m_biases[i] + (1 - beta1) * bias_grads[i];
        v_biases[i] = beta2 * v_biases[i] + (1 - beta2) * bias_grads[i] * bias_grads[i];
        float m_hat = m_biases[i] / (1 - std::pow(beta1, t));
        float v_hat = v_biases[i] / (1 - std::pow(beta2, t));
        biases[i] -= learning_rate * m_hat / (std::sqrt(v_hat) + epsilon);
    }
}
#ifndef OPTIMIZER_H
#define OPTIMIZER_H

#include <vector>

class Optimizer {
public:
    virtual ~Optimizer() {}
    virtual void update(
        std::vector<std::vector<float>>& weights,
        const std::vector<std::vector<float>>& weight_grads,
        std::vector<float>& biases,
        const std::vector<float>& bias_grads
    ) = 0;
};

#endif // OPTIMIZER_H
#ifndef ADAM_H
#define ADAM_H

#include "Optimizer.h"
#include <vector>

class Adam : public Optimizer {
public:
    Adam(float learning_rate, float beta1, float beta2, float epsilon);
    void update(
        std::vector<std::vector<float>>& weights,
        const std::vector<std::vector<float>>& weight_grads,
        std::vector<float>& biases,
        const std::vector<float>& bias_grads
    ) override;

private:
    float learning_rate;
    float beta1;
    float beta2;
    float epsilon;
    int t;
    std::vector<std::vector<float>> m_weights;
    std::vector<std::vector<float>> v_weights;
    std::vector<float> m_biases;
    std::vector<float> v_biases;
};

#endif // ADAM_H
#include "Optimizer.h"

class SGD : public Optimizer {
public:
    SGD(float learning_rate);
    void update(std::vector<std::vector<float>>& weights,
                const std::vector<std::vector<float>>& weight_grads,
                std::vector<float>& biases,
                const std::vector<float>& bias_grads) override;
private:
    float lr;
};
#ifndef RECURRENTLAYER_H
#define RECURRENTLAYER_H

#include <vector>

class RecurrentLayer {
public:
    RecurrentLayer(int input_size, int hidden_size);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);
private:
    int input_size;
    int hidden_size;
    std::vector<std::vector<float>> weights_input;
    std::vector<std::vector<float>> weights_hidden;
    std::vector<float> biases;
    std::vector<float> hidden_state;
};

#endif // RECURRENTLAYER_H
#ifndef DENSELAYER_H
#define DENSELAYER_H

#include <vector>

class DenseLayer {
public:
    DenseLayer(int input_size, int output_size);
    
    // Forward and Backward Pass
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);

    // Const Accessors for weights and biases
    const std::vector<std::vector<float>>& get_weights() const;
    const std::vector<float>& get_biases() const;

    // Non-const Accessors for weights and biases (for updating)
    std::vector<std::vector<float>>& get_weights();
    std::vector<float>& get_biases();

    // Const Accessors for gradients
    const std::vector<std::vector<float>>& get_weight_gradients() const;
    const std::vector<float>& get_bias_gradients() const;

    // Non-const Accessors for gradients (for accumulating gradients)
    std::vector<std::vector<float>>& get_weight_gradients();
    std::vector<float>& get_bias_gradients();

    // Reset gradients after parameter updates
    void reset_gradients();

private:
    int input_size;
    int output_size;
    std::vector<std::vector<float>> weights;
    std::vector<float> biases;
    std::vector<float> input;

    // Gradients
    std::vector<std::vector<float>> weight_gradients;
    std::vector<float> bias_gradients;
};

#endif // DENSELAYER_H
#ifndef CONVOLUTIONLAYER_H
#define CONVOLUTIONLAYER_H

#include <vector>

class ConvolutionLayer {
public:
    ConvolutionLayer(int input_channels, int output_channels, int kernel_size);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);
private:
    int input_channels;
    int output_channels;
    int kernel_size;
    std::vector<std::vector<float>> kernels;
    std::vector<float> biases;
    std::vector<float> input;
};

#endif // CONVOLUTIONLAYER_H
#include "ConvolutionLayer.h"
#include <cstdlib>

ConvolutionLayer::ConvolutionLayer(int in_ch, int out_ch, int k_size)
    : input_channels(in_ch), output_channels(out_ch), kernel_size(k_size),
      kernels(out_ch, std::vector<float>(in_ch * kernel_size * kernel_size)),
      biases(out_ch, 0.0f) {
    // Initialize kernels and biases
    for(auto &kernel : kernels) {
        for(auto &w : kernel) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
}

std::vector<float> ConvolutionLayer::forward(const std::vector<float>& input_data) {
    input = input_data;
    // Simple convolution operation (placeholder)
    std::vector<float> output(output_channels, 0.0f);
    for(int o = 0; o < output_channels; ++o) {
        for(int i = 0; i < input_channels * kernel_size * kernel_size; ++i) {
            output[o] += kernels[o][i] * input_data[i];
        }
        output[o] += biases[o];
    }
    return output;
}

std::vector<float> ConvolutionLayer::backward(const std::vector<float>& grad_output) {
    std::vector<float> grad_input(input_channels * kernel_size * kernel_size, 0.0f);
    // Simple backward operation (placeholder)
    for(int o = 0; o < output_channels; ++o) {
        for(int i = 0; i < input_channels * kernel_size * kernel_size; ++i) {
            grad_input[i] += kernels[o][i] * grad_output[o];
            // Update kernels and biases here if implementing training
        }
    }
    return grad_input;
}
#include "DenseLayer.h"
#include <cstdlib>
#include <algorithm>
#include <cmath>

// Constructor with Xavier Initialization
DenseLayer::DenseLayer(int input_sz, int output_sz)
        : input_size(input_sz), output_size(output_sz),
          weights(output_sz, std::vector<float>(input_sz)),
          biases(output_sz, 0.0f),
          weight_gradients(output_sz, std::vector<float>(input_sz, 0.0f)),
          bias_gradients(output_sz, 0.0f) {
    // Xavier Initialization
    float limit = std::sqrt(6.0f / (input_size + output_size));
    for(auto &row : weights) {
        for(auto &w : row) {
            w = ((static_cast<float>(rand()) / RAND_MAX) * 2 * limit) - limit;
        }
    }
}

// Forward Pass
std::vector<float> DenseLayer::forward(const std::vector<float>& input_data) {
    input = input_data;
    std::vector<float> output(output_size, 0.0f);
    for(int i = 0; i < output_size; ++i) {
        for(int j = 0; j < input_size; ++j) {
            output[i] += weights[i][j] * input[j];
        }
        output[i] += biases[i];
    }
    return output;
}

// Backward Pass
std::vector<float> DenseLayer::backward(const std::vector<float>& grad_output) {
    // Compute gradients w.r.t weights and biases
    for(int i = 0; i < output_size; ++i) {
        for(int j = 0; j < input_size; ++j) {
            weight_gradients[i][j] += grad_output[i] * input[j];
        }
        bias_gradients[i] += grad_output[i];
    }

    // Compute gradient w.r.t input to propagate to previous layers
    std::vector<float> grad_input(input_size, 0.0f);
    for(int j = 0; j < input_size; ++j) {
        for(int i = 0; i < output_size; ++i) {
            grad_input[j] += weights[i][j] * grad_output[i];
        }
    }
    return grad_input;
}

// Const Accessors
const std::vector<std::vector<float>>& DenseLayer::get_weights() const {
    return weights;
}

const std::vector<float>& DenseLayer::get_biases() const {
    return biases;
}

const std::vector<std::vector<float>>& DenseLayer::get_weight_gradients() const {
    return weight_gradients;
}

const std::vector<float>& DenseLayer::get_bias_gradients() const {
    return bias_gradients;
}

// Non-const Accessors
std::vector<std::vector<float>>& DenseLayer::get_weights() {
    return weights;
}

std::vector<float>& DenseLayer::get_biases() {
    return biases;
}

std::vector<std::vector<float>>& DenseLayer::get_weight_gradients() {
    return weight_gradients;
}

std::vector<float>& DenseLayer::get_bias_gradients() {
    return bias_gradients;
}

// Reset Gradients
void DenseLayer::reset_gradients() {
    for(auto& wg : weight_gradients) {
        std::fill(wg.begin(), wg.end(), 0.0f);
    }
    std::fill(bias_gradients.begin(), bias_gradients.end(), 0.0f);
}
#ifndef TRANSFORMERLAYER_H
#define TRANSFORMERLAYER_H

#include <vector>

class TransformerLayer {
public:
    TransformerLayer(int model_size, int num_heads);
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& grad_output);
private:
    int model_size;
    int num_heads;
    // Transformer-specific components
};

#endif // TRANSFORMERLAYER_H
#include "TransformerLayer.h"

TransformerLayer::TransformerLayer(int m_size, int n_heads)
    : model_size(m_size), num_heads(n_heads) {}

std::vector<float> TransformerLayer::forward(const std::vector<float>& input) {
    // Placeholder for transformer forward pass
    return input;
}

std::vector<float> TransformerLayer::backward(const std::vector<float>& grad_output) {
    // Placeholder for transformer backward pass
    return grad_output;
}
#include "RecurrentLayer.h"
#include <cstdlib>
#include <valarray>
#include <cstddef> // Add this line

RecurrentLayer::RecurrentLayer(int in_size, int hid_size)
        : input_size(in_size), hidden_size(hid_size),
          weights_input(hid_size, std::vector<float>(in_size)),
          weights_hidden(hid_size, std::vector<float>(hid_size)),
          biases(hid_size, 0.0f),
          hidden_state(hid_size, 0.0f) {
    // Initialize weights and biases
    for (auto &row : weights_input) {
        for (auto &w : row) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
    for (auto &row : weights_hidden) {
        for (auto &w : row) {
            w = static_cast<float>(rand()) / RAND_MAX;
        }
    }
}

std::vector<float> RecurrentLayer::forward(const std::vector<float>& input_data) {
    std::vector<float> output(hidden_size, 0.0f);
    for (int h = 0; h < hidden_size; ++h) {
        for (int i = 0; i < input_size; ++i) {
            output[h] += weights_input[h][i] * input_data[i];
        }
        for (int h_prev = 0; h_prev < hidden_size; ++h_prev) {
            output[h] += weights_hidden[h][h_prev] * hidden_state[h_prev];
        }
        output[h] += biases[h];
        // Apply activation (e.g., tanh) here if needed
        hidden_state[h] = std::tanh(output[h]);
    }
    return hidden_state;
}

std::vector<float> RecurrentLayer::backward(const std::vector<float>& grad_output) {
    (void)grad_output; // Mark as intentionally unused

    // Placeholder: Implement backward logic
    return std::vector<float>();
}
#ifndef SIGMOID_H
#define SIGMOID_H

#include <vector>

class Sigmoid {
public:
    std::vector<float> activate(const std::vector<float>& input);
    std::vector<float> derivative(const std::vector<float>& input);
};

#endif // SIGMOID_H
#ifndef RELU_H
#define RELU_H

#include <vector>

class ReLU {
public:
    std::vector<float> activate(const std::vector<float>& input);
    std::vector<float> derivative(const std::vector<float>& input);
};

#endif // RELU_H
// Softmax.cpp
#include "Softmax.h"
#include <cmath>
#include <algorithm>

namespace Activation {

    std::vector<float> Softmax::derivative(const std::vector<float>& output) const {
        // Implementation of the derivative
        std::vector<float> derivative(output.size(), 0.0f);
        for (size_t i = 0; i < output.size(); ++i) {
            derivative[i] = output[i] * (1.0f - output[i]);
        }
        return derivative;
    }

    std::vector<float> Softmax::activate(const std::vector<float>& input) {
        std::vector<float> exps;
        float max_val = *std::max_element(input.begin(), input.end());
        float sum = 0.0f;
        for (auto val : input) {
            float e = std::exp(val - max_val);
            exps.push_back(e);
            sum += e;
        }
        std::vector<float> output;
        for (auto e : exps) {
            output.push_back(e / sum);
        }
        return output;
    }
} // namespace Activation
#include "Tanh.h"
#include <cmath>

std::vector<float> Tanh::activate(const std::vector<float>& input) {
    std::vector<float> output;
    for (auto val : input) {
        output.push_back(std::tanh(val));
    }
    return output;
}

std::vector<float> Tanh::derivative(const std::vector<float>& input) {
    std::vector<float> derivatives;
    for (auto val : input) {
        float tanh_val = std::tanh(val);
        derivatives.push_back(1.0f - tanh_val * tanh_val);
    }
    return derivatives;
}
#include "Sigmoid.h"
#include <cmath>

std::vector<float> Sigmoid::activate(const std::vector<float>& input) {
    std::vector<float> output;
    for (auto val : input) {
        output.push_back(1.0f / (1.0f + std::exp(-val)));
    }
    return output;
}

std::vector<float> Sigmoid::derivative(const std::vector<float>& input) {
    std::vector<float> derivatives;
    for (auto val : input) {
        float sigmoid = 1.0f / (1.0f + std::exp(-val));
        derivatives.push_back(sigmoid * (1 - sigmoid));
    }
    return derivatives;
}
#ifndef TANH_H
#define TANH_H

#include <vector>

class Tanh {
public:
    std::vector<float> activate(const std::vector<float>& input);
    std::vector<float> derivative(const std::vector<float>& input);
};

#endif // TANH_H
#ifndef SOFTMAX_H
#define SOFTMAX_H

#include <vector>

namespace Activation {

    class Softmax {
    public:
        std::vector<float> activate(const std::vector<float> &input);

        std::vector<float> derivative(const std::vector<float>& output) const;
    };

} // namespace Activation

#endif // SOFTMAX_H
#include "ReLU.h"

std::vector<float> ReLU::activate(const std::vector<float>& input) {
    std::vector<float> output;
    for (auto val : input) {
        output.push_back(val > 0 ? val : 0);
    }
    return output;
}

std::vector<float> ReLU::derivative(const std::vector<float>& input) {
    std::vector<float> derivatives;
    for (auto val : input) {
        derivatives.push_back(val > 0 ? 1.0f : 0.0f);
    }
    return derivatives;
}
#include "NeuralNetwork.h"
#include "Optimizers/Adam.h"
#include <cmath>
#include <iostream>
#include <fstream>

// Constructor with Adam Optimizer
NeuralNetwork::NeuralNetwork()
        : optimizer(std::make_unique<Adam>(0.001f, 0.9f, 0.999f, 1e-8f)), logger(INFO) { // Using Adam optimizer
    // Initialize Dense Layers
    dense_layers.emplace_back(DenseLayer(100, 50));
    dense_layers.emplace_back(DenseLayer(50, 10));

    // Initialize Gradient Storage based on layers
    for(auto &layer : dense_layers) {
        weight_grads.emplace_back(layer.get_weight_gradients()); // Each layer's weight gradients
        bias_grads.emplace_back(layer.get_bias_gradients());     // Each layer's bias gradients
    }
}

// Forward Pass through the Network
std::vector<float> NeuralNetwork::forward(const std::vector<float>& input) {
    std::vector<float> output = input;
    output = dense_layers[0].forward(output);
    output = relu.activate(output);
    output = dense_layers[1].forward(output);
    output = softmax.activate(output);
    return output;
}

// Backward Pass through the Network
std::vector<float> NeuralNetwork::backward(const std::vector<float>& loss_grad) {
    // Reset gradients before accumulating new ones
    for (std::size_t i = 0; i < dense_layers.size(); ++i) {
        // Reset weight gradients
        for(auto &wg : weight_grads[i]) {
            std::fill(wg.begin(), wg.end(), 0.0f);
        }
        // Reset bias gradients
        std::fill(bias_grads[i].begin(), bias_grads[i].end(), 0.0f);
    }

    // Backward Pass
    std::vector<float> grad = softmax.derivative(loss_grad);
    grad = dense_layers[1].backward(grad);
    grad = relu.derivative(grad);
    grad = dense_layers[0].backward(grad);

    // Collect gradients from each layer
    for (std::size_t i = 0; i < dense_layers.size(); ++i) {
        auto& layer_weight_grads = dense_layers[i].get_weight_gradients();
        auto& layer_bias_grads = dense_layers[i].get_bias_gradients();
        weight_grads[i] = layer_weight_grads;
        bias_grads[i] = layer_bias_grads;
    }

    return grad;
}

// Update Network Parameters using Optimizer
void NeuralNetwork::update_parameters() {
    // Collect all weights and biases and their gradients
    std::vector<std::vector<float>> all_weights;
    std::vector<std::vector<float>> all_weight_grads;
    std::vector<float> all_biases;
    std::vector<float> all_bias_grads;

    for (std::size_t i = 0; i < dense_layers.size(); ++i) {
        auto& w = dense_layers[i].get_weights(); // std::vector<std::vector<float>>
        auto& b = dense_layers[i].get_biases();  // std::vector<float>
        auto& wg = weight_grads[i];              // std::vector<std::vector<float>>
        auto& bg = bias_grads[i];                // std::vector<float>

        // Flatten weights and weight gradients
        for(auto& weight_vec : w) {
            all_weights.emplace_back(weight_vec);
        }
        for(auto& weight_grad_vec : wg) {
            all_weight_grads.emplace_back(weight_grad_vec);
        }

        // Flatten biases and bias gradients
        for(auto& bias : b) {
            all_biases.emplace_back(bias);
        }
        for(auto& bias_grad : bg) {
            all_bias_grads.emplace_back(bias_grad);
        }
    }

    // Update using optimizer
    optimizer->update(all_weights, all_weight_grads, all_biases, all_bias_grads);

    // Distribute updated weights and biases back to layers
    size_t idx_w = 0;
    size_t idx_b = 0;
    for(auto& layer : dense_layers) {
        auto& w = layer.get_weights();
        auto& b = layer.get_biases();
        // Assign weights
        for(auto& weight_vec : w) {
            weight_vec = all_weights[idx_w++];
        }
        // Assign biases
        for(auto& bias : b) {
            bias = all_biases[idx_b++];
        }
    }

    // Reset the gradients after the update
    for (std::size_t i = 0; i < dense_layers.size(); ++i) {
        // Reset weight gradients
        for(auto &wg : weight_grads[i]) {
            std::fill(wg.begin(), wg.end(), 0.0f);
        }
        // Reset bias gradients
        std::fill(bias_grads[i].begin(), bias_grads[i].end(), 0.0f);
    }

    logger.log("NeuralNetwork: Parameters updated successfully.");
}

// Save the Model to a Binary File
void NeuralNetwork::save(const std::string& path) {
    std::ofstream ofs(path, std::ios::binary);
    if (!ofs.is_open()) {
        logger.log("Failed to open file for saving: " + path, ERROR);
        return;
    }

    // Save weights and biases
    for (const auto& layer : dense_layers) {
        const auto& weights = layer.get_weights();   // Const accessor
        const auto& biases = layer.get_biases();     // Const accessor

        // Save weights
        for (const auto& neuron_weights : weights) {
            ofs.write(reinterpret_cast<const char*>(neuron_weights.data()), neuron_weights.size() * sizeof(float));
        }

        // Save biases
        ofs.write(reinterpret_cast<const char*>(biases.data()), biases.size() * sizeof(float));
    }

    ofs.close();
    logger.log("NeuralNetwork: Model saved to " + path);
}

// Load the Model from a Binary File
void NeuralNetwork::load(const std::string& path) {
    std::ifstream ifs(path, std::ios::binary);
    if (!ifs.is_open()) {
        logger.log("Failed to open file for loading: " + path, ERROR);
        return;
    }

    // Load weights and biases
    for (auto& layer : dense_layers) {
        auto& weights = layer.get_weights();   // Non-const accessor
        auto& biases = layer.get_biases();     // Non-const accessor

        // Load weights
        for (auto& neuron_weights : weights) {
            ifs.read(reinterpret_cast<char*>(neuron_weights.data()), neuron_weights.size() * sizeof(float));
        }

        // Load biases
        ifs.read(reinterpret_cast<char*>(biases.data()), biases.size() * sizeof(float));
    }

    ifs.close();
    logger.log("NeuralNetwork: Model loaded from " + path);
}
#ifndef NEURALNETWORK_H
#define NEURALNETWORK_H

#include <vector>
#include <memory>
#include <string>
#include "Layers/DenseLayer.h"
#include "Layers/ConvolutionLayer.h"
#include "Layers/RecurrentLayer.h"
#include "Layers/TransformerLayer.h"
#include "Activation/ReLU.h"
#include "Activation/Sigmoid.h"
#include "Activation/Softmax.h"
#include "Activation/Tanh.h"
#include "Optimizers/Optimizer.h"
#include "../utils/Logger.h"

class NeuralNetwork {
public:
    NeuralNetwork();
    std::vector<float> forward(const std::vector<float>& input);
    std::vector<float> backward(const std::vector<float>& loss_grad);
    void update_parameters();

    // Save and load model parameters
    void save(const std::string& path);
    void load(const std::string& path);

private:
    std::vector<DenseLayer> dense_layers;
    ReLU relu;
    Sigmoid sigmoid;
    Activation::Softmax softmax;
    Tanh tanh;
    std::unique_ptr<Optimizer> optimizer;
    Logger logger;

    // Corrected Gradient Storage
    std::vector<std::vector<std::vector<float>>> weight_grads; // [layer][neuron][weight]
    std::vector<std::vector<float>> bias_grads;               // [layer][bias]
};

#endif // NEURALNETWORK_H
// Logger.cpp
#include "Logger.h"
#include <iostream>
#include <fstream>
#include <ctime>

Logger::Logger(LogLevel level) : current_level(level) {}

void Logger::log(const std::string& message, LogLevel level) {
    if(level < current_level) return;

    std::time_t now = std::time(nullptr);
    char buf[100];
    std::strftime(buf, sizeof(buf), "%Y-%m-%d %H:%M:%S", std::localtime(&now));
    std::string log_message = std::string(buf) + " - " + message;

    switch(level) {
        case INFO:
            std::cout << "[INFO] " << log_message << std::endl;
            break;
        case DEBUG:
            std::cout << "[DEBUG] " << log_message << std::endl;
            break;
        case ERROR:
            std::cerr << "[ERROR] " << log_message << std::endl;
            break;
    }

    // Optionally, write to a log file
}
// Config.cpp
#include "Config.h"
#include <fstream>
#include <stdexcept>

Config::Config(const std::string& config_path) {
    std::ifstream infile(config_path);
    if (!infile.is_open()) {
        throw std::runtime_error("Failed to open config file.");
    }
    infile >> config_json;
    infile.close();
}

template<typename T>
T Config::get(const std::string& key) const {
    if(config_json.contains(key)) {
        return config_json[key].get<T>();
    }
    throw std::runtime_error("Key not found in config.");
}

// Explicit template instantiation
template int Config::get<int>(const std::string&) const;
template float Config::get<float>(const std::string&) const;
template std::string Config::get<std::string>(const std::string&) const;
// Add more as needed
#include "FileUtils.h"
#include <sys/stat.h>

bool FileUtils::file_exists(const std::string& path) {
    struct stat buffer;
    return (stat (path.c_str(), &buffer) == 0);
}
// Config.h
#ifndef CONFIG_H
#define CONFIG_H

#include <string>
#include <nlohmann/json.hpp>

class Config {
public:
    Config(const std::string& config_path);

    template<typename T>
    T get(const std::string& key) const;

private:
    nlohmann::json config_json;
};

#endif // CONFIG_H
// Logger.h
#ifndef LOGGER_H
#define LOGGER_H

#include <string>

enum LogLevel {
    INFO,
    DEBUG,
    ERROR
};

class Logger {
public:
    Logger(LogLevel level = INFO);
    void log(const std::string& message, LogLevel level = INFO);
private:
    LogLevel current_level;
};

#endif // LOGGER_H
#ifndef FILEUTILS_H
#define FILEUTILS_H

#include <string>

class FileUtils {
public:
    static bool file_exists(const std::string& path);
};

#endif // FILEUTILS_H
#include "interface/InterfaceEngine.h"
#include <pybind11/embed.h>

namespace py = pybind11;

int main() {
    py::scoped_interpreter guard{};
    py::module::import("InterfaceEngine"); // Correct module name
    return 0;
}
